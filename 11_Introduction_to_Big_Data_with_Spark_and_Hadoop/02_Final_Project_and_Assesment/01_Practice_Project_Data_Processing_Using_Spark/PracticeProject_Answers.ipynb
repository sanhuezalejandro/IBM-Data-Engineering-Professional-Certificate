{"cells":[{"cell_type":"markdown","id":"2becd71c-4911-4a0d-8f7e-135aa18af0dc","metadata":{},"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"ca80512b-e889-4e53-bc97-6e8cdeaf2fab","metadata":{},"source":["# Practice Project\n","\n","Estimated time needed: **60** minutes\n","\n","This practice project focuses on data transformation and integration using PySpark. You will work with two datasets, perform various transformations such as adding columns, renaming columns, dropping unnecessary columns, joining dataframes, and finally, writing the results into both a Hive warehouse and an HDFS file system.\n"]},{"cell_type":"markdown","id":"75f4f7ec-02f1-43ad-be35-83857c4ad056","metadata":{},"source":["### Prerequisites \n","\n","For this lab assignment, you will use wget, Python and Spark (PySpark). Therefore, it's essential to make sure that the below-specified libraries are installed in your lab environment or within Skills Network (SN) Labs.  \n","\n"," \n"]},{"cell_type":"raw","id":"2690314b-fa7b-47e6-8b4c-efc2523fac15","metadata":{"vscode":{"languageId":"raw"}},"source":["# Installing required packages\n","\n","!pip install wget pyspark  findspark"]},{"cell_type":"markdown","id":"a8860f53-2549-45dc-8199-a59b2dc6ac0f","metadata":{},"source":["#### Prework - Initiate the Spark Session\n"]},{"cell_type":"code","execution_count":1,"id":"943a3bef-2d17-43d0-bf97-326dc251dbd8","metadata":{},"outputs":[],"source":["import findspark\n","\n","findspark.init()"]},{"cell_type":"code","execution_count":2,"id":"412adadd-09c1-41e7-b66e-bbd1780771a9","metadata":{},"outputs":[],"source":["# PySpark is the Spark API for Python. In this lab, we use PySpark to initialize the SparkContext.   \n","\n","from pyspark import SparkContext, SparkConf\n","\n","from pyspark.sql import SparkSession"]},{"cell_type":"code","execution_count":3,"id":"bae4b7b2-fa1d-488d-9852-b3f9f79c84f8","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","24/06/01 14:17:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"]}],"source":["# Creating a SparkContext object\n","\n","sc = SparkContext.getOrCreate()\n","\n","# Creating a Spark Session\n","\n","spark = SparkSession \\\n","    .builder \\\n","    .appName(\"Python Spark DataFrames basic example\") \\\n","    .config(\"spark.some.config.option\", \"some-value\") \\\n","    .getOrCreate()"]},{"cell_type":"markdown","id":"1fea069e-82fc-4518-88a7-15d04c829e51","metadata":{},"source":["### Task 1: Load datasets into PySpark DataFrames\n","\n","Download the datasets from the folloing links using `wget` and load it in a Spark Dataframe.\n","\n","1. https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset1.csv  \n","2. https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset2.csv  \n","\n","*Hint: Import wget*\n"]},{"cell_type":"code","execution_count":4,"id":"f5a18d6f-b4b7-4b6c-9739-5f27aa18b5ad","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  4115  100  4115    0     0   3577      0  0:00:01  0:00:01 --:--:--  3578\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  2688  100  2688    0     0   3745      0 --:--:-- --:--:-- --:--:--  37430      0 --:--:-- --:--:-- --:--:--     0\n"]}],"source":["#download dataset using wget\n","!curl -O https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset1.csv\n","!curl -O https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset2.csv"]},{"cell_type":"markdown","id":"207e2b81-9a98-4be4-b4ba-4d1e2a4fe716","metadata":{},"source":["\n","<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","# download the dataset using wget\n","import wget\n","\n","link_to_data1 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset1.csv'\n","wget.download(link_to_data1)\n","\n","link_to_data2 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset2.csv'\n","wget.download(link_to_data2)\n","```\n","\n","</details>\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["'dataset2 (1).csv'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import wget\n","\n","link_to_data1 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset1.csv'\n","wget.download(link_to_data1)\n","\n","link_to_data2 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset2.csv'\n","wget.download(link_to_data2)"]},{"cell_type":"code","execution_count":7,"id":"e471c768-2dd6-4f97-8cb1-93cb4923d61c","metadata":{},"outputs":[],"source":["#load the data into a pyspark dataframe\n","df1 = spark.read.csv(\"dataset1.csv\", header=True, inferSchema=True)\n","df2 = spark.read.csv(\"dataset2.csv\", header=True, inferSchema=True)\n"]},{"cell_type":"markdown","id":"21944044-bc23-4c93-ba4b-b248191b6e8b","metadata":{},"source":["\n","<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","\n","#load the data into a pyspark dataframe\n","    \n","df1 = spark.read.csv(\"dataset1.csv\", header=True, inferSchema=True)\n","df2 = spark.read.csv(\"dataset2.csv\", header=True, inferSchema=True)\n","```\n","\n","</details>\n","\n","\n"]},{"cell_type":"markdown","id":"a5698be3-1c6d-4a5c-8a5c-8c8ec4f7a63b","metadata":{},"source":["### Task 2: Display the schema of both dataframes\n","\n","Display the schema of `df1` and `df2` to understand the structure of the datasets.\n"]},{"cell_type":"code","execution_count":8,"id":"fa160ca9-2e29-494f-9958-06c4c056dbed","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- customer_id: integer (nullable = true)\n"," |-- date_column: string (nullable = true)\n"," |-- amount: integer (nullable = true)\n"," |-- description: string (nullable = true)\n"," |-- location: string (nullable = true)\n","\n","root\n"," |-- customer_id: integer (nullable = true)\n"," |-- transaction_date: string (nullable = true)\n"," |-- value: integer (nullable = true)\n"," |-- notes: string (nullable = true)\n","\n"]}],"source":["#print the schema of df1 and df2\n","df1.printSchema()\n","df2.printSchema()\n"]},{"cell_type":"markdown","id":"97326756-6609-49d7-8001-0af7fcd3018b","metadata":{},"source":["\n","<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","\n","#print the schema of df1 and df2\n","    \n","df1.printSchema()\n","df2.printSchema()\n","```\n","\n","</details>\n","\n","\n"]},{"cell_type":"markdown","id":"dd7bad10-5e80-456d-be89-db20f51c214e","metadata":{},"source":["#### Task 3: Add a new column to each dataframe\n","\n","Add a new column named **year** to `df1` and **quarter** to `df2` representing the year and quarter of the data.\n","\n","*Hint: use withColumn. Convert the date columns which are present as string to date before extracting the year and quarter information*\n","\n","\n"]},{"cell_type":"code","execution_count":9,"id":"44d998b4-c66a-4e27-b584-aec80cd31b1d","metadata":{},"outputs":[],"source":["from pyspark.sql.functions import year, quarter, to_date\n","\n","#Add new column year to df1\n","df1 = df1.withColumn('year', year(to_date('date_column','dd/MM/yyyy')))\n","    \n","#Add new column quarter to df2    \n","df2 = df2.withColumn('quarter', quarter(to_date('transaction_date','dd/MM/yyyy')))\n"]},{"cell_type":"markdown","id":"ccd44b7c-0aa8-4a07-9b21-cd0f734fa0ec","metadata":{},"source":["\n","<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","from pyspark.sql.functions import year, quarter\n","\n","#Add new column year to df1\n","df1 = df1.withColumn('year', year(to_date('date_column','dd/MM/yyyy')))\n","    \n","#Add new column quarter to df2    \n","df2 = df2.withColumn('quarter', quarter(to_date('transaction_date','dd/MM/yyyy')))```\n","\n","</details>\n","\n","\n","\n"]},{"cell_type":"markdown","id":"3bbcb5b5-2a59-4499-9ea1-38c01796c937","metadata":{},"source":["#### Task 4: Rename columns in both dataframes\n","\n","Rename the column **amount** to **transaction_amount** in `df1` and **value** to **transaction_value** in `df2`.\n","\n","*Hint: Use withColumnRenamed*\n"]},{"cell_type":"code","execution_count":10,"id":"f38c98f2-1fbe-431b-8104-406bf70a0085","metadata":{},"outputs":[],"source":["#Rename df1 column amount to transaction_amount\n","df1 = df1.withColumnRenamed('amount', 'transaction_amount')\n","    \n","#Rename df2 column value to transaction_value\n","df2 = df2.withColumnRenamed('value', 'transaction_value')\n","\n"]},{"cell_type":"markdown","id":"2aafcc32-ca34-4841-bb27-373ad8801f50","metadata":{},"source":["\n","\n","<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","\n","#Rename df1 column amount to transaction_amount\n","df1 = df1.withColumnRenamed('amount', 'transaction_amount')\n","    \n","#Rename df2 column value to transaction_value\n","df2 = df2.withColumnRenamed('value', 'transaction_value')\n","```\n","\n","</details>\n","\n","\n","\n"]},{"cell_type":"markdown","id":"3e66645d-bd34-446c-b776-0868aa748e45","metadata":{},"source":["#### Task 5: Drop unnecessary columns\n","\n","Drop the columns **description** and **location** from `df1` and **notes** from `df2`.\n","\n","\n"]},{"cell_type":"code","execution_count":11,"id":"50801f33-31ed-4f61-9bd1-474459a76083","metadata":{},"outputs":[],"source":["#Drop columns description and location from df1\n","df1 = df1.drop('description', 'location')\n","    \n","#Drop column notes from df2\n","df2 = df2.drop('notes')\n","\n"]},{"cell_type":"markdown","id":"1a47a95e-fd77-464f-87b3-cda646abf426","metadata":{},"source":["\n","\n","\n","<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","\n","#Drop columns description and location from df1\n","df1 = df1.drop('description', 'location')\n","    \n","#Drop column notes from df2\n","df2 = df2.drop('notes')\n","```\n","\n","</details>\n","\n","\n","\n"]},{"cell_type":"markdown","id":"586ca479-8dcb-43a9-a544-26a6e181dce9","metadata":{},"source":["#### Task 6: Join dataframes based on a common column\n","\n","Join `df1` and `df2` based on the common column **customer_id** and create a new dataframe named `joined_df`.\n","\n","\n"]},{"cell_type":"code","execution_count":12,"id":"5f997e28-9fe6-4a59-bb96-16a786ea7bd2","metadata":{},"outputs":[],"source":["#join df1 and df2 based on common column customer_id\n","joined_df = df1.join(df2, 'customer_id', 'inner')\n"]},{"cell_type":"markdown","id":"8c5dda03-03d8-4b27-892b-99bc576f8835","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","\n","#join df1 and df2 based on common column customer_id\n","joined_df = df1.join(df2, 'customer_id', 'inner')\n","    \n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"f393b919-8206-4959-bf1a-b03c6a2b85e0","metadata":{},"source":["#### Task 7: Filter data based on a condition\n","\n","Filter `joined_df` to include only transactions where \"transaction_amount\" is greater than 1000 and create a new dataframe named `filtered_df`.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":13,"id":"f07e03ff-c8ba-4d30-b95b-626c40c55a86","metadata":{},"outputs":[],"source":["# filter the dataframe for transaction amount > 1000\n","filtered_df = joined_df.filter(\"transaction_amount > 1000\")    \n"]},{"cell_type":"markdown","id":"a2092929-6eb8-44ec-b122-5981b69bc738","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","\n","# filter the dataframe for transaction amount > 1000\n","filtered_df = joined_df.filter(\"transaction_amount > 1000\")    \n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"28eb840a-b5b0-4215-a8b8-230e775fa9c9","metadata":{},"source":["#### Task 8: Aggregate data by customer\n","\n","Calculate the total transaction amount for each customer in `filtered_df` and display the result.\n","\n","*Hint: Use sum from pyspark.sql.functions*\n"]},{"cell_type":"code","execution_count":14,"id":"de64f8fa-35d0-43f0-9458-f5660ebc10ce","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+------------+\n","|customer_id|total_amount|\n","+-----------+------------+\n","|         31|        3200|\n","|         85|        1800|\n","|         78|        1500|\n","|         34|        1200|\n","|         81|        5500|\n","|         28|        2600|\n","|         76|        2600|\n","|         27|        4200|\n","|         91|        3200|\n","|         22|        1200|\n","|         93|        5500|\n","|          1|        5000|\n","|         52|        2600|\n","|         13|        4800|\n","|          6|        4500|\n","|         16|        2600|\n","|         40|        2600|\n","|         94|        1200|\n","|         57|        5500|\n","|         54|        1500|\n","+-----------+------------+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.sql.functions import sum\n","   \n","# group by customer_id and aggregate the sum of transaction amount\n","\n","total_amount_per_customer = filtered_df.groupBy('customer_id').agg(sum('transaction_amount').alias('total_amount'))\n","\n","#display the result\n","total_amount_per_customer.show()\n","\n"]},{"cell_type":"markdown","id":"3b4e0ca3-b978-467b-9656-9562bb93de4c","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","\n","from pyspark.sql.functions import sum\n","   \n","# group by customer_id and aggregate the sum of transaction amount\n","\n","total_amount_per_customer = filtered_df.groupBy('customer_id').agg(sum('transaction_amount').alias('total_amount'))\n","\n","#display the result\n","total_amount_per_customer.show()\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"0608951a-a4c1-4383-accc-31440c7d2497","metadata":{},"source":["#### Task 9: Write the result to a Hive table\n","\n","Write `total_amount_per_customer` to a Hive table named **customer_totals**.\n"]},{"cell_type":"code","execution_count":15,"id":"6122e459-de47-45f1-8d66-2e30269a6ea3","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Write total_amount_per_customer to a Hive table named customer_totals\n","total_amount_per_customer.write.mode(\"overwrite\").saveAsTable(\"customer_totals\")\n"]},{"cell_type":"markdown","id":"28451a08-bc1b-4cc0-9b7d-80fd94265fd1","metadata":{},"source":["\n","<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","\n","# Write total_amount_per_customer to a Hive table named customer_totals\n","total_amount_per_customer.write.mode(\"overwrite\").saveAsTable(\"customer_totals\")\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"4fdf7da0-7ed2-4335-b9db-0ffe59db82b7","metadata":{},"source":["#### Task 10: Write the filtered data to HDFS\n","\n","Write `filtered_df` to HDFS in parquet format to a file named **filtered_data**.\n"]},{"cell_type":"code","execution_count":18,"id":"aa3bbf83-7133-49bb-acae-60b6267b0c0a","metadata":{},"outputs":[],"source":["# Set the time parser policy to \"LEGACY\" to maintain the behavior of parsing dates \n","# as it was before Spark 3.0. This is necessary because Spark 3.0 introduced a \n","# stricter date parsing mechanism which may cause errors with certain date formats.\n","# ISO (yyyy-MM-dd) and we are using 'dd/MM/yyyy'\n","spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n","\n","#Write filtered_df to HDFS in parquet format file filtered_data\n","\n","filtered_df.write.mode(\"overwrite\").parquet(\"filtered_data.parquet\")\n","\n","#ALTERNATIVE\n","# Suppose your DataFrame has a column called \"date\" that needs to be converted\n","# filtered_df = filtered_df.withColumn(\"date\", to_date(col(\"date\"), \"MM/dd/yyyy\"))\n"]},{"cell_type":"markdown","id":"ce1fe65b-bde0-444c-9f53-56b3ffc6d02b","metadata":{},"source":["\n","<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","\n","#Write filtered_df to HDFS in parquet format file filtered_data\n","\n","filtered_df.write.mode(\"overwrite\").parquet(\"filtered_data.parquet\")\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"e5e0e30a-511a-4a98-937d-cf1fd924d8fd","metadata":{},"source":["#### Task 11: Add a new column based on a condition\n","\n","Add a new column named **high_value** to `df1` indicating whether the transaction_amount is greater than 5000. When the value is greater than 5000, the value of the column should be **Yes**. When the value is less than or equal to 5000, the value of the column should be **No**. \n","\n","*Hint: Use when and lit from pyspark.sql.functions\n"]},{"cell_type":"code","execution_count":19,"id":"08a7c942-e970-4daf-ab7d-a9d352092e10","metadata":{},"outputs":[],"source":["from pyspark.sql.functions import when, lit\n","\n","# Add new column with value indicating whether transaction amount is > 5000 or not\n","df1 = df1.withColumn(\"high_value\", when(df1.transaction_amount > 5000, lit(\"Yes\")).otherwise(lit(\"No\")))\n"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+-----------+------------------+----+----------+\n","|customer_id|date_column|transaction_amount|year|high_value|\n","+-----------+-----------+------------------+----+----------+\n","|          1|   1/1/2022|              5000|2022|        No|\n","|          2|  15/2/2022|              1200|2022|        No|\n","|          3|  20/3/2022|               800|2022|        No|\n","|          4|  10/4/2022|              3000|2022|        No|\n","|          5|   5/5/2022|              6000|2022|       Yes|\n","+-----------+-----------+------------------+----+----------+\n","only showing top 5 rows\n","\n"]}],"source":["df1.show(5)"]},{"cell_type":"markdown","id":"c818856b-db28-45e6-b45b-9d19b299d09c","metadata":{},"source":["\n","<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","\n","from pyspark.sql.functions import when, lit\n","\n","# Add new column with value indicating whether transaction amount is > 5000 or not\n","df1 = df1.withColumn(\"high_value\", when(df1.transaction_amount > 5000, lit(\"Yes\")).otherwise(lit(\"No\")))\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"9d016c4d-0738-479c-b2c7-9efed216275d","metadata":{},"source":["#### Task 12: Calculate the average transaction value per quarter\n","\n","Calculate and display the average transaction value for each quarter in `df2` and create a new dataframe named `average_value_per_quarter` with column `avg_trans_val`.\n","\n","*Hint: Use avg from pyspark.sql.functions*\n"]},{"cell_type":"code","execution_count":20,"id":"df36dbb4-0785-4619-833c-e31e04dc9761","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+------------------+\n","|quarter|     avg_trans_val|\n","+-------+------------------+\n","|      1| 1111.111111111111|\n","|      3|1958.3333333333333|\n","|      4| 816.6666666666666|\n","|      2|            1072.0|\n","+-------+------------------+\n","\n"]}],"source":["from pyspark.sql.functions import avg\n","\n","#calculate the average transaction value for each quarter in df2\n","average_value_per_quarter = df2.groupBy('quarter').agg(avg(\"transaction_value\").alias(\"avg_trans_val\"))\n","\n","    \n","#show the average transaction value for each quarter in df2    \n","average_value_per_quarter.show()\n"]},{"cell_type":"markdown","id":"0a6bfadb-d5db-42ea-8a52-14ec996b8800","metadata":{},"source":["\n","<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","from pyspark.sql.functions import avg\n","\n","#calculate the average transaction value for each quarter in df2\n","average_value_per_quarter = df2.groupBy('quarter').agg(avg(\"transaction_value\").alias(\"avg_trans_val\"))\n","\n","    \n","#show the average transaction value for each quarter in df2    \n","average_value_per_quarter.show()\n","\n","```\n","\n","</details>\n"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-------------+\n","|quarter|avg_trans_val|\n","+-------+-------------+\n","|      1|       1111.1|\n","|      3|       1958.3|\n","|      4|        816.7|\n","|      2|       1072.0|\n","+-------+-------------+\n","\n"]}],"source":["from pyspark.sql.functions import avg, round\n","\n","# Calculate the average transaction value for each quarter in df2 and round to 1 decimal place\n","average_value_per_quarter = df2.groupBy('quarter').agg(round(avg(\"transaction_value\"), 1).alias(\"avg_trans_val\"))\n","\n","# Show the average transaction value for each quarter in df2\n","average_value_per_quarter.show()\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-------------+\n","|quarter|avg_trans_val|\n","+-------+-------------+\n","|      1|       1111.1|\n","|      2|       1072.0|\n","|      3|       1958.3|\n","|      4|        816.7|\n","+-------+-------------+\n","\n"]}],"source":["from pyspark.sql.functions import avg, round\n","\n","# Calculate the average transaction value for each quarter in df2 and round to 1 decimal place\n","average_value_per_quarter = df2.groupBy('quarter').agg(round(avg(\"transaction_value\"), 1).alias(\"avg_trans_val\"))\n","\n","# Order the results by the quarter\n","average_value_per_quarter = average_value_per_quarter.orderBy('quarter')\n","\n","# Show the average transaction value for each quarter in df2\n","average_value_per_quarter.show()"]},{"cell_type":"markdown","id":"c54b6053-2c0a-4443-8f43-81f65c26658f","metadata":{},"source":["#### Task 13: Write the result to a Hive table\n","\n","Write `average_value_per_quarter` to a Hive table named **quarterly_averages**.\n"]},{"cell_type":"code","execution_count":31,"id":"a1e3820b-2c2e-47f7-b40f-c6cba1f26db5","metadata":{},"outputs":[],"source":["#Write average_value_per_quarter to a Hive table named quarterly_averages\n","\n","average_value_per_quarter.write.mode(\"overwrite\").saveAsTable(\"quarterly_averages\")\n"]},{"cell_type":"markdown","id":"d52e506a-8730-4a16-b0aa-2f45cb084024","metadata":{},"source":["\n","<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","\n","#Write average_value_per_quarter to a Hive table named quarterly_averages\n","\n","average_value_per_quarter.write.mode(\"overwrite\").saveAsTable(\"quarterly_averages\")\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"fb249232-24f9-490c-b9cc-c6178f0f1a0b","metadata":{},"source":["#### Task 14: Calculate the total transaction value per year\n","\n","Calculate and display the total transaction value for each year in `df1` and create a new dataframe named `total_value_per_year` with column `total_transaction_val`.\n"]},{"cell_type":"code","execution_count":32,"id":"a5eec426-3efd-4598-bb21-8afcc89ed47e","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+---------------------+\n","|year|total_transaction_val|\n","+----+---------------------+\n","|2022|                29800|\n","|2023|                28100|\n","|2024|                25700|\n","|2025|                25700|\n","|2026|                25700|\n","|2027|                25700|\n","|2028|                25700|\n","|2029|                25700|\n","|2030|                 9500|\n","+----+---------------------+\n","\n"]}],"source":["# calculate the total transaction value for each year in df1.\n","total_value_per_year = df1.groupBy('year').agg(sum(\"transaction_amount\").alias(\"total_transaction_val\"))\n","\n","total_value_per_year = total_value_per_year.orderBy('year')\n","\n","# show the total transaction value for each year in df1.\n","total_value_per_year.show()\n"]},{"cell_type":"markdown","id":"51384a08-3363-4844-b6c3-6a32267cd25e","metadata":{},"source":["\n","<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","\n","# calculate the total transaction value for each year in df1.\n","total_value_per_year = df1.groupBy('year').agg(sum(\"transaction_amount\").alias(\"total_transaction_val\"))\n","\n","# show the total transaction value for each year in df1.\n","total_value_per_year.show()\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"aad05286-af6c-4ce8-ad67-1c75a9a97a84","metadata":{},"source":["#### Task 15: Write the result to HDFS\n","\n","Write `total_value_per_year` to HDFS in the CSV format to file named **total_value_per_year**.\n","\n"]},{"cell_type":"code","execution_count":33,"id":"72652e0d-de5d-41d0-8589-ed135971f92c","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["24/06/03 21:37:15 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 243339 ms exceeds timeout 120000 ms\n","24/06/03 21:37:15 WARN SparkContext: Killing executors is not supported by current scheduler.\n","24/06/03 21:37:24 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/03 21:37:24 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/03 21:37:34 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:37:34 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:37:44 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:37:44 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:37:54 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:37:54 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:38:04 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:38:04 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:38:14 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:38:14 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:38:24 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:38:24 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:38:34 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:38:34 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:38:44 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:38:44 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:38:54 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:38:54 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:39:04 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:39:04 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:39:14 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:39:14 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:39:24 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:39:24 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:39:34 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:39:34 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:39:44 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:39:44 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:39:54 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:39:54 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:40:04 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:40:04 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:40:14 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:40:14 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:40:24 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:40:24 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:40:34 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:40:34 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:40:44 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:40:44 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:40:54 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:40:54 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:41:04 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:41:04 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:41:14 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:41:14 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:41:24 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:41:24 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:41:34 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:41:34 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:41:44 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:41:44 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:41:54 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:41:54 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:42:04 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:42:04 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:42:14 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:42:14 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:42:24 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:42:24 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:42:34 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:42:34 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:42:44 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:42:44 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:42:54 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:42:54 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:43:04 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:43:04 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:43:14 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:43:14 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:43:24 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:43:24 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:43:34 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:43:34 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:43:44 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:43:44 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:43:54 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:43:54 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:44:04 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:44:04 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:44:14 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:44:14 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:44:24 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:44:24 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:44:34 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:44:34 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:44:44 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:44:44 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:44:54 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:44:54 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:45:04 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/03 21:45:04 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/03 21:45:14 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:45:14 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:45:24 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/03 21:45:24 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/03 21:45:34 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:45:34 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:45:44 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:45:44 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:45:54 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/03 21:45:54 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/03 21:46:04 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:46:04 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:46:14 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:46:14 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:46:24 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:46:24 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:46:34 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:46:34 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:46:44 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:46:44 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:46:54 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:46:54 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:47:04 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:47:04 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:47:14 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:47:14 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:49971\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/03 21:47:14 ERROR Executor: Exit as unable to send heartbeats to driver more than 60 times\n"]}],"source":["#Write total_value_per_year to HDFS in the CSV format\n","\n","total_value_per_year.write.mode(\"overwrite\").csv(\"total_value_per_year.csv\")"]},{"cell_type":"markdown","id":"bea8be35-0a14-4b05-8d20-6bae5ce34fa7","metadata":{},"source":["\n","<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","\n","#Write total_value_per_year to HDFS in the CSV format\n","\n","total_value_per_year.write.mode(\"overwrite\").csv(\"total_value_per_year.csv\")\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"56229a00-8f61-494c-985c-2ba4714c8232","metadata":{},"source":["### Congratulations! You have completed the lab.\n","This practice project provides hands-on experience with data transformation and integration using PySpark. You've performed various tasks, including adding columns, renaming columns, dropping unnecessary columns, joining dataframes, and writing the results into both a Hive warehouse and an HDFS file system.\n"]},{"cell_type":"markdown","id":"b8701778-e7ae-4ec7-b989-1e7cea2b5698","metadata":{},"source":["## Authors\n","\n","Raghul Ramesh\n","\n","Lavanya T S\n"]},{"cell_type":"markdown","id":"23839b8a-3d86-462a-a1d3-29483d541cec","metadata":{},"source":["<!--## Change Log -->\n"]},{"cell_type":"markdown","id":"0cfb33a5-3116-4a06-acd8-a6a3b8e00c3d","metadata":{},"source":["<!--|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n","|-|-|-|-|\n","|2023-09-01|0.1|Lavanya T S|Initial version|\n","|2023-09-08|0.2|Pornima More|QA pass with edits|-->\n"]},{"cell_type":"markdown","id":"e83e5d33-8193-4216-bcab-729b62d8b75f","metadata":{},"source":["<h3 align=\"center\"> &#169; IBM Corporation. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"prev_pub_hash":"11f72aae77d8016a27f044c7c8c6c15789e27d81d5809ee65ea885d8da7126a7"},"nbformat":4,"nbformat_minor":4}
