{"cells":[{"cell_type":"markdown","id":"2ff80d08-4efb-4576-a6ba-3607157ed139","metadata":{},"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"03af1847-010b-4385-be36-a2d78b6e55a7","metadata":{},"source":["# Final Project: Data Analysis using Spark\n","\n","Estimated time needed: **60** minutes\n","\n","This final project is similar to the Practice Project you did. In this project, you will not be provided with hints or solutions. You will create a DataFrame by loading data from a CSV file and apply transformations and actions using Spark SQL. This needs to be achieved by performing the following tasks:\n","\n","- Task 1: Generate DataFrame from CSV data.\n","- Task 2: Define a schema for the data.\n","- Task 3: Display schema of DataFrame.\n","- Task 4: Create a temporary view.\n","- Task 5: Execute an SQL query.\n","- Task 6: Calculate Average Salary by Department.\n","- Task 7: Filter and Display IT Department Employees.\n","- Task 8: Add 10% Bonus to Salaries.\n","- Task 9: Find Maximum Salary by Age.\n","- Task 10: Self-Join on Employee Data.\n","- Task 11: Calculate Average Employee Age.\n","- Task 12: Calculate Total Salary by Department.\n","- Task 13: Sort Data by Age and Salary.\n","- Task 14: Count Employees in Each Department.\n","- Task 15: Filter Employees with the letter o in the Name.\n"]},{"cell_type":"markdown","id":"795324d6-5dd8-43a1-ba14-41907d46a188","metadata":{},"source":["### Prerequisites \n","\n","1. For this lab assignment, you will be using Python and Spark (PySpark). Therefore, it's essential to make sure that the following libraries are installed in your lab environment or within Skills Network (SN) Labs\n"]},{"cell_type":"code","execution_count":1,"id":"ff775a2a-8b3f-4fd9-adcf-6ba3d125ebd7","metadata":{},"outputs":[],"source":["# Installing required packages  \n","\n","#!pip install pyspark  findspark wget\n"]},{"cell_type":"code","execution_count":2,"id":"eb56c76d-5976-4a4c-9fc0-3781cabdda20","metadata":{},"outputs":[],"source":["import findspark\n","\n","findspark.init()"]},{"cell_type":"code","execution_count":3,"id":"606c1ba1-8fbe-4f23-a9d5-8fd1b00b7a3b","metadata":{},"outputs":[],"source":["# PySpark is the Spark API for Python. In this lab, we use PySpark to initialize the SparkContext.   \n","\n","from pyspark import SparkContext, SparkConf\n","\n","from pyspark.sql import SparkSession"]},{"cell_type":"code","execution_count":4,"id":"38ddefbc-13d1-4ec1-9f10-b0d1534fb84f","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["24/06/01 18:40:27 WARN Utils: Your hostname, Alejandros-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.10 instead (on interface en0)\n","24/06/01 18:40:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n","Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","24/06/01 18:40:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","24/06/01 18:40:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"]}],"source":["# Creating a SparkContext object  \n","\n","sc = SparkContext.getOrCreate()\n","\n","# Creating a SparkSession  \n","\n","spark = SparkSession \\\n","    .builder \\\n","    .appName(\"Python Spark DataFrames basic example\") \\\n","    .config(\"spark.some.config.option\", \"some-value\") \\\n","    .getOrCreate()"]},{"cell_type":"markdown","id":"2df9d7f9-7c8b-4ff9-8108-d1b103523fd2","metadata":{},"source":["2. Download the CSV data.  \n"]},{"cell_type":"code","execution_count":5,"id":"81f43113-63af-4677-b02e-21bc9ae57123","metadata":{},"outputs":[{"data":{"text/plain":["'employees.csv'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Download the CSV data first into a local `employees.csv` file\n","import wget\n","wget.download(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/data/employees.csv\")"]},{"cell_type":"markdown","id":"08ed1d05-c8b8-4e1a-8eeb-ce6ae0348389","metadata":{},"source":["### Tasks\n"]},{"cell_type":"markdown","id":"f3d616e9-1643-461e-8643-8d8a6fae372d","metadata":{},"source":["#### Task 1: Generate a Spark DataFrame from the CSV data\n","\n","Read data from the provided CSV file, `employees.csv` and import it into a Spark DataFrame variable named `employees_df`.\n","\n"," \n"]},{"cell_type":"code","execution_count":9,"id":"ae9f8cef-4fb3-4f4a-a4ca-66f11af0f7fd","metadata":{},"outputs":[],"source":["# Read data from the \"emp\" CSV file and import it into a DataFrame variable named \"employees_df\"  \n","employees_df = spark.read.csv(\"employees.csv\", header=True)"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["#employees_df = spark.read.csv(\"employees.csv\", header=True, inferSchema=True)"]},{"cell_type":"markdown","id":"7fb386c8-407d-42e6-bdb6-5ba328bc58b9","metadata":{},"source":["#### Task 2: Define a schema for the data\n","\n","Construct a schema for the input data and then utilize the defined schema to read the CSV file to create a DataFrame named `employees_df`.  \n"]},{"cell_type":"code","execution_count":40,"id":"a5fae738-3189-4a90-8763-f9e18b6cbaad","metadata":{},"outputs":[],"source":["# Define a Schema for the input data and read the file using the user-defined Schema\n","\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n","\n","# Define the schema for the employees.csv\n","schema = StructType([\n","    StructField(\"Emp_No\", IntegerType(), True),\n","    StructField(\"Emp_Name\", StringType(), True),\n","    StructField(\"Salary\", IntegerType(), True),\n","    StructField(\"Age\", IntegerType(), True),\n","    StructField(\"Department\", StringType(), True)\n","])\n","\n","# Read the CSV file using the defined schema\n","employees_df = spark.read.csv(\"employees.csv\", header=True, schema=schema)\n"]},{"cell_type":"markdown","id":"71ac10a7-4a18-40d3-baf7-f97e514440f8","metadata":{},"source":["#### Task 3: Display schema of DataFrame\n","\n","Display the schema of the `employees_df` DataFrame, showing all columns and their respective data types.  \n"]},{"cell_type":"code","execution_count":41,"id":"72d7a48d-2a65-4826-a2c6-014df029a893","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Emp_No: integer (nullable = true)\n"," |-- Emp_Name: string (nullable = true)\n"," |-- Salary: integer (nullable = true)\n"," |-- Age: integer (nullable = true)\n"," |-- Department: string (nullable = true)\n","\n"]}],"source":["# Display all columns of the DataFrame, along with their respective data types\n","employees_df.printSchema()"]},{"cell_type":"markdown","id":"9b2ce4b1-cb83-4a1b-a538-6470878ec530","metadata":{},"source":["#### Task 4: Create a temporary view\n","\n","Create a temporary view named `employees` for the `employees_df` DataFrame, enabling Spark SQL queries on the data. \n"]},{"cell_type":"code","execution_count":42,"id":"d905eb32-bd7d-445d-8f95-8e5c6f26cca7","metadata":{},"outputs":[],"source":["# Create a temporary view named \"employees\" for the DataFrame\n","employees_df.createOrReplaceTempView(\"employees\")\n"]},{"cell_type":"markdown","id":"e96e2132-96f4-489f-9308-907c629ce9ea","metadata":{},"source":["#### Task 5: Execute an SQL query\n","\n","Compose and execute an SQL query to fetch the records from the `employees` view where the age of employees exceeds 30. Then, display the result of the SQL query, showcasing the filtered records.\n"]},{"cell_type":"code","execution_count":44,"id":"fc2aa96a-afb7-4ab0-8c21-c82dd6f87e59","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+-----------+------+---+----------+\n","|Emp_No|   Emp_Name|Salary|Age|Department|\n","+------+-----------+------+---+----------+\n","|   199|    Douglas|  2600| 34|     Sales|\n","|   200|   Jennifer|  4400| 36| Marketing|\n","|   201|    Michael| 13000| 32|        IT|\n","|   202|        Pat|  6000| 39|        HR|\n","|   203|      Susan|  6500| 36| Marketing|\n","|   205|    Shelley| 12008| 33|   Finance|\n","|   206|    William|  8300| 37|        IT|\n","|   100|     Steven| 24000| 39|        IT|\n","|   102|        Lex| 17000| 37| Marketing|\n","|   103|  Alexander|  9000| 39| Marketing|\n","|   104|      Bruce|  6000| 38|        IT|\n","|   105|      David|  4800| 39|        IT|\n","|   106|      Valli|  4800| 38|     Sales|\n","|   107|      Diana|  4200| 35|     Sales|\n","|   109|     Daniel|  9000| 35|        HR|\n","|   110|       John|  8200| 31| Marketing|\n","|   111|     Ismael|  7700| 32|        IT|\n","|   112|Jose Manuel|  7800| 34|        HR|\n","|   113|       Luis|  6900| 34|     Sales|\n","|   116|     Shelli|  2900| 37|   Finance|\n","+------+-----------+------+---+----------+\n","only showing top 20 rows\n","\n"]}],"source":["# SQL query to fetch solely the records from the View where the age exceeds 30\n","result_df = spark.sql(\"SELECT * FROM employees WHERE Age > 30\")\n","\n","# Show the results of the query\n","result_df.show()"]},{"cell_type":"markdown","id":"ef503f99-9225-4631-8aa8-58435222be3c","metadata":{},"source":["#### Task 6: Calculate Average Salary by Department\n","\n","Compose an SQL query to retrieve the average salary of employees grouped by department. Display the result.\n"]},{"cell_type":"code","execution_count":45,"id":"2ed5bda9-2041-48f6-a847-94e8438b6bff","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+-----------------+\n","|Department|        AvgSalary|\n","+----------+-----------------+\n","|     Sales|5492.923076923077|\n","|        HR|           5837.5|\n","|   Finance|           5730.8|\n","| Marketing|6633.333333333333|\n","|        IT|           7400.0|\n","+----------+-----------------+\n","\n"]}],"source":["# SQL query to calculate the average salary of employees grouped by department\n","result_df = spark.sql(\"SELECT Department, AVG(Salary) AS AvgSalary FROM employees GROUP BY Department\")\n","\n","# Show the results of the query\n","result_df.show()"]},{"cell_type":"markdown","id":"85327c81-b404-4efe-ae74-3418af3dbf47","metadata":{},"source":["#### Task 7: Filter and Display IT Department Employees\n","\n","Apply a filter on the `employees_df` DataFrame to select records where the department is `'IT'`. Display the filtered DataFrame.\n"]},{"cell_type":"code","execution_count":47,"id":"08d8c238-b4d4-4c71-811f-95ba82b75236","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+--------+------+---+----------+\n","|Emp_No|Emp_Name|Salary|Age|Department|\n","+------+--------+------+---+----------+\n","|   198|  Donald|  2600| 29|        IT|\n","|   201| Michael| 13000| 32|        IT|\n","|   206| William|  8300| 37|        IT|\n","|   100|  Steven| 24000| 39|        IT|\n","|   104|   Bruce|  6000| 38|        IT|\n","|   105|   David|  4800| 39|        IT|\n","|   111|  Ismael|  7700| 32|        IT|\n","|   129|   Laura|  3300| 38|        IT|\n","|   132|      TJ|  2100| 34|        IT|\n","|   136|   Hazel|  2200| 29|        IT|\n","+------+--------+------+---+----------+\n","\n"]}],"source":["# Apply a filter to select records where the department is 'IT'\n","it_employees_df = employees_df.filter(employees_df[\"Department\"] == \"IT\")\n","\n","# Display the filtered DataFrame\n","it_employees_df.show()"]},{"cell_type":"markdown","id":"14f5a92f-f7fb-4f4c-8b79-98b2d283b328","metadata":{},"source":["#### Task 8: Add 10% Bonus to Salaries\n","\n","Perform a transformation to add a new column named \"SalaryAfterBonus\" to the DataFrame. Calculate the new salary by adding a 10% bonus to each employee's salary.\n"]},{"cell_type":"code","execution_count":48,"id":"9e0c1219-962f-4e64-9431-b85a95e781dc","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+---------+------+---+----------+------------------+\n","|Emp_No| Emp_Name|Salary|Age|Department|  SalaryAfterBonus|\n","+------+---------+------+---+----------+------------------+\n","|   198|   Donald|  2600| 29|        IT|2860.0000000000005|\n","|   199|  Douglas|  2600| 34|     Sales|2860.0000000000005|\n","|   200| Jennifer|  4400| 36| Marketing|            4840.0|\n","|   201|  Michael| 13000| 32|        IT|14300.000000000002|\n","|   202|      Pat|  6000| 39|        HR| 6600.000000000001|\n","|   203|    Susan|  6500| 36| Marketing| 7150.000000000001|\n","|   204|  Hermann| 10000| 29|   Finance|           11000.0|\n","|   205|  Shelley| 12008| 33|   Finance|13208.800000000001|\n","|   206|  William|  8300| 37|        IT|            9130.0|\n","|   100|   Steven| 24000| 39|        IT|26400.000000000004|\n","|   101|    Neena| 17000| 27|     Sales|           18700.0|\n","|   102|      Lex| 17000| 37| Marketing|           18700.0|\n","|   103|Alexander|  9000| 39| Marketing|            9900.0|\n","|   104|    Bruce|  6000| 38|        IT| 6600.000000000001|\n","|   105|    David|  4800| 39|        IT|            5280.0|\n","|   106|    Valli|  4800| 38|     Sales|            5280.0|\n","|   107|    Diana|  4200| 35|     Sales|            4620.0|\n","|   108|    Nancy| 12008| 28|     Sales|13208.800000000001|\n","|   109|   Daniel|  9000| 35|        HR|            9900.0|\n","|   110|     John|  8200| 31| Marketing|            9020.0|\n","+------+---------+------+---+----------+------------------+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.sql.functions import col\n","\n","# Add a new column \"SalaryAfterBonus\" with 10% bonus added to the original salary\n","employees_with_bonus_df = employees_df.withColumn(\"SalaryAfterBonus\", col(\"Salary\") * 1.1)\n","\n","# Display the DataFrame with the new column\n","employees_with_bonus_df.show()"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+---------+------+---+----------+----------------+\n","|Emp_No| Emp_Name|Salary|Age|Department|SalaryAfterBonus|\n","+------+---------+------+---+----------+----------------+\n","|   198|   Donald|  2600| 29|        IT|            2860|\n","|   199|  Douglas|  2600| 34|     Sales|            2860|\n","|   200| Jennifer|  4400| 36| Marketing|            4840|\n","|   201|  Michael| 13000| 32|        IT|           14300|\n","|   202|      Pat|  6000| 39|        HR|            6600|\n","|   203|    Susan|  6500| 36| Marketing|            7150|\n","|   204|  Hermann| 10000| 29|   Finance|           11000|\n","|   205|  Shelley| 12008| 33|   Finance|           13209|\n","|   206|  William|  8300| 37|        IT|            9130|\n","|   100|   Steven| 24000| 39|        IT|           26400|\n","|   101|    Neena| 17000| 27|     Sales|           18700|\n","|   102|      Lex| 17000| 37| Marketing|           18700|\n","|   103|Alexander|  9000| 39| Marketing|            9900|\n","|   104|    Bruce|  6000| 38|        IT|            6600|\n","|   105|    David|  4800| 39|        IT|            5280|\n","|   106|    Valli|  4800| 38|     Sales|            5280|\n","|   107|    Diana|  4200| 35|     Sales|            4620|\n","|   108|    Nancy| 12008| 28|     Sales|           13209|\n","|   109|   Daniel|  9000| 35|        HR|            9900|\n","|   110|     John|  8200| 31| Marketing|            9020|\n","+------+---------+------+---+----------+----------------+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.sql.functions import round\n","\n","# Perform a transformation to add a new column \"SalaryAfterBonus\" with a 10% bonus, rounded to the nearest integer\n","employees_with_bonus_df = employees_df.withColumn(\"SalaryAfterBonus\", round(col(\"Salary\") * 1.1).cast(\"int\"))\n","\n","# Display the DataFrame with the new column\n","employees_with_bonus_df.show()"]},{"cell_type":"markdown","id":"cd2bcea1-4de1-425f-b9c1-03675ce7c315","metadata":{},"source":["#### Task 9: Find Maximum Salary by Age\n","\n","Group the data by age and calculate the maximum salary for each age group. Display the result.\n","\n"]},{"cell_type":"code","execution_count":51,"id":"7307d690-fedc-4b24-b128-44698cf72ce1","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+---------+\n","|Age|MaxSalary|\n","+---+---------+\n","| 26|     3600|\n","| 27|    17000|\n","| 28|    12008|\n","| 29|    10000|\n","| 30|     8000|\n","| 31|     8200|\n","| 32|    13000|\n","| 33|    12008|\n","| 34|     7800|\n","| 35|     9000|\n","| 36|     7900|\n","| 37|    17000|\n","| 38|     6000|\n","| 39|    24000|\n","+---+---------+\n","\n"]}],"source":["from pyspark.sql.functions import max\n","\n","# Group the data by age and calculate the maximum salary for each age group\n","max_salary_by_age_df = employees_df.groupBy(\"Age\").agg(max(\"Salary\").alias(\"MaxSalary\"))\n","\n","max_salary_by_age_df = max_salary_by_age_df.orderBy(\"Age\")\n","\n","# Display the result\n","max_salary_by_age_df.show()\n"]},{"cell_type":"markdown","id":"a83cbdd0-9dbc-40bf-ab98-adc525a1da89","metadata":{},"source":["#### Task 10: Self-Join on Employee Data\n","\n","Join the \"employees_df\" DataFrame with itself based on the \"Emp_No\" column. Display the result.\n"]},{"cell_type":"code","execution_count":52,"id":"0339f4a3-37d1-4c4e-8e9c-3939f9637f03","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+---------+------+---+----------+---------+------+---+----------+\n","|Emp_No| Emp_Name|Salary|Age|Department| Emp_Name|Salary|Age|Department|\n","+------+---------+------+---+----------+---------+------+---+----------+\n","|   198|   Donald|  2600| 29|        IT|   Donald|  2600| 29|        IT|\n","|   199|  Douglas|  2600| 34|     Sales|  Douglas|  2600| 34|     Sales|\n","|   200| Jennifer|  4400| 36| Marketing| Jennifer|  4400| 36| Marketing|\n","|   201|  Michael| 13000| 32|        IT|  Michael| 13000| 32|        IT|\n","|   202|      Pat|  6000| 39|        HR|      Pat|  6000| 39|        HR|\n","|   203|    Susan|  6500| 36| Marketing|    Susan|  6500| 36| Marketing|\n","|   204|  Hermann| 10000| 29|   Finance|  Hermann| 10000| 29|   Finance|\n","|   205|  Shelley| 12008| 33|   Finance|  Shelley| 12008| 33|   Finance|\n","|   206|  William|  8300| 37|        IT|  William|  8300| 37|        IT|\n","|   100|   Steven| 24000| 39|        IT|   Steven| 24000| 39|        IT|\n","|   101|    Neena| 17000| 27|     Sales|    Neena| 17000| 27|     Sales|\n","|   102|      Lex| 17000| 37| Marketing|      Lex| 17000| 37| Marketing|\n","|   103|Alexander|  9000| 39| Marketing|Alexander|  9000| 39| Marketing|\n","|   104|    Bruce|  6000| 38|        IT|    Bruce|  6000| 38|        IT|\n","|   105|    David|  4800| 39|        IT|    David|  4800| 39|        IT|\n","|   106|    Valli|  4800| 38|     Sales|    Valli|  4800| 38|     Sales|\n","|   107|    Diana|  4200| 35|     Sales|    Diana|  4200| 35|     Sales|\n","|   108|    Nancy| 12008| 28|     Sales|    Nancy| 12008| 28|     Sales|\n","|   109|   Daniel|  9000| 35|        HR|   Daniel|  9000| 35|        HR|\n","|   110|     John|  8200| 31| Marketing|     John|  8200| 31| Marketing|\n","+------+---------+------+---+----------+---------+------+---+----------+\n","only showing top 20 rows\n","\n"]}],"source":["# Join the DataFrame with itself based on the \"Emp_No\" column\n","joined_df = employees_df.alias(\"df1\").join(employees_df.alias(\"df2\"), \"Emp_No\")\n","\n","# Display the result\n","joined_df.show()"]},{"cell_type":"markdown","id":"e606e268-18b6-4eac-b4a9-415883a541ae","metadata":{},"source":["#### Task 11: Calculate Average Employee Age\n","\n","Calculate the average age of employees using the built-in aggregation function. Display the result.\n"]},{"cell_type":"code","execution_count":53,"id":"8fc046ec-9134-4916-95ea-dba4c7b0d61a","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+\n","|AverageAge|\n","+----------+\n","|     33.56|\n","+----------+\n","\n"]}],"source":["# Calculate the average age of employees\n","from pyspark.sql.functions import avg \n","\n","# Calculate the average age of employees\n","average_age_df = employees_df.agg(avg(\"Age\").alias(\"AverageAge\"))\n","\n","# Display the result\n","average_age_df.show()"]},{"cell_type":"markdown","id":"29be5221-1b31-4330-8f1d-e2a7dc9bae41","metadata":{},"source":["#### Task 12: Calculate Total Salary by Department\n","\n","Calculate the total salary for each department using the built-in aggregation function. Display the result.\n"]},{"cell_type":"code","execution_count":54,"id":"c14c2fae-3bec-4b49-9af0-ca545c586d45","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+-----------+\n","|Department|TotalSalary|\n","+----------+-----------+\n","|     Sales|      71408|\n","|        HR|      46700|\n","|   Finance|      57308|\n","| Marketing|      59700|\n","|        IT|      74000|\n","+----------+-----------+\n","\n"]}],"source":["# Calculate the total salary for each department. Hint - User GroupBy and Aggregate functions\n","from pyspark.sql.functions import sum \n","\n","# Calculate the total salary for each department\n","total_salary_per_department_df = employees_df.groupBy(\"Department\").agg(sum(\"Salary\").alias(\"TotalSalary\"))\n","\n","# Display the result\n","total_salary_per_department_df.show()"]},{"cell_type":"markdown","id":"1b8f2021-2a6f-437f-8493-cb812a59815d","metadata":{},"source":["#### Task 13: Sort Data by Age and Salary\n","\n","Apply a transformation to sort the DataFrame by age in ascending order and then by salary in descending order. Display the sorted DataFrame.\n"]},{"cell_type":"code","execution_count":56,"id":"7a530351-d480-4d82-8bf1-31fd919a5e44","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+---------+------+---+----------+\n","|Emp_No| Emp_Name|Salary|Age|Department|\n","+------+---------+------+---+----------+\n","|   137|   Renske|  3600| 26| Marketing|\n","|   101|    Neena| 17000| 27|     Sales|\n","|   114|      Den| 11000| 27|   Finance|\n","|   108|    Nancy| 12008| 28|     Sales|\n","|   130|    Mozhe|  2800| 28| Marketing|\n","|   126|    Irene|  2700| 28|        HR|\n","|   204|  Hermann| 10000| 29|   Finance|\n","|   115|Alexander|  3100| 29|   Finance|\n","|   134|  Michael|  2900| 29|     Sales|\n","|   198|   Donald|  2600| 29|        IT|\n","|   140|   Joshua|  2500| 29|   Finance|\n","|   136|    Hazel|  2200| 29|        IT|\n","|   120|  Matthew|  8000| 30|        HR|\n","|   110|     John|  8200| 31| Marketing|\n","|   127|    James|  2400| 31|        HR|\n","|   201|  Michael| 13000| 32|        IT|\n","|   111|   Ismael|  7700| 32|        IT|\n","|   119|    Karen|  2500| 32|   Finance|\n","|   205|  Shelley| 12008| 33|   Finance|\n","|   124|    Kevin|  5800| 33| Marketing|\n","+------+---------+------+---+----------+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.sql.functions import desc\n","\n","# Apply a transformation to sort the DataFrame by age in ascending order and then by salary in descending order\n","sorted_df = employees_df.orderBy(\"Age\", desc(\"Salary\"))\n","\n","# Display the sorted DataFrame\n","sorted_df.show()"]},{"cell_type":"markdown","id":"4949fc08-a65f-454c-b226-d94b5b8672f9","metadata":{},"source":["#### Task 14: Count Employees in Each Department\n","\n","Calculate the number of employees in each department. Display the result.\n"]},{"cell_type":"code","execution_count":57,"id":"76108370-264c-4d11-8aa7-8b977d0e40c6","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+-------------+\n","|Department|EmployeeCount|\n","+----------+-------------+\n","|     Sales|           13|\n","|        HR|            8|\n","|   Finance|           10|\n","| Marketing|            9|\n","|        IT|           10|\n","+----------+-------------+\n","\n"]}],"source":["from pyspark.sql.functions import count\n","\n","# Calculate the number of employees in each department\n","employee_count_per_department_df = employees_df.groupBy(\"Department\").agg(count(\"*\").alias(\"EmployeeCount\"))\n","\n","# Display the result\n","employee_count_per_department_df.show()"]},{"cell_type":"markdown","id":"909fe445-e57c-43f4-8604-84a422abc72c","metadata":{},"source":["#### Task 15: Filter Employees with the letter o in the Name\n","\n","Apply a filter to select records where the employee's name contains the letter `'o'`. Display the filtered DataFrame.\n"]},{"cell_type":"code","execution_count":59,"id":"a2ca2d20-e23d-42d4-96c5-4d78fbc9ca1e","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+-----------+------+---+----------+\n","|Emp_No|   Emp_Name|Salary|Age|Department|\n","+------+-----------+------+---+----------+\n","|   198|     Donald|  2600| 29|        IT|\n","|   199|    Douglas|  2600| 34|     Sales|\n","|   110|       John|  8200| 31| Marketing|\n","|   112|Jose Manuel|  7800| 34|        HR|\n","|   130|      Mozhe|  2800| 28| Marketing|\n","|   133|      Jason|  3300| 38|     Sales|\n","|   139|       John|  2700| 36|     Sales|\n","|   140|     Joshua|  2500| 29|   Finance|\n","+------+-----------+------+---+----------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["24/06/01 22:24:50 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 153043 ms exceeds timeout 120000 ms\n","24/06/01 22:24:50 WARN SparkContext: Killing executors is not supported by current scheduler.\n","24/06/01 22:24:51 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:24:51 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:25:01 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:25:01 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:25:21 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:25:21 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:25:31 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:25:31 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:25:41 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:25:41 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:25:51 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:25:51 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:26:01 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:26:01 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:26:11 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:26:11 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:26:21 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:26:21 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:26:31 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:26:31 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:26:41 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:26:41 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:26:51 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:26:51 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:27:01 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:27:01 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:27:11 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:27:11 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:27:21 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:27:21 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:27:31 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:27:31 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:27:41 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:27:41 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:27:51 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:27:51 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:28:01 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:28:01 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:28:11 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:28:11 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:28:21 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:28:21 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:28:31 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:28:31 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:28:41 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:28:41 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:28:51 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:28:51 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:29:01 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:29:01 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:29:11 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:29:11 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:29:21 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:29:21 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:29:31 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:29:31 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:29:41 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:29:41 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:29:51 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:29:51 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:30:01 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:30:01 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:30:11 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:30:11 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:30:21 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:30:21 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:30:31 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:30:31 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:30:41 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:30:41 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:30:51 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:30:51 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:31:01 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:31:01 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:31:11 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:31:11 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:31:21 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:31:21 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:31:31 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:31:31 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:31:41 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:31:41 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:31:51 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:31:51 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:32:01 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:32:01 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:32:11 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:32:11 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:32:21 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:32:21 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:32:31 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:32:31 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:32:41 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:32:41 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:32:51 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:32:51 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:33:01 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:33:01 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:33:11 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:33:11 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:33:21 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:33:21 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:33:31 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:33:31 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:33:41 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:33:41 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:33:51 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:33:51 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:34:01 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:34:01 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:34:11 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:34:11 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:34:21 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:34:21 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:34:31 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:34:31 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:34:41 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:34:41 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n","\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n","\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n","\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n","\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n","\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n","\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n","\tat scala.concurrent.Promise.complete(Promise.scala:53)\n","\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n","\tat scala.concurrent.Promise.success(Promise.scala:86)\n","\tat scala.concurrent.Promise.success$(Promise.scala:86)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n","\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n","\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n","\t... 8 more\n","24/06/01 22:34:51 ERROR Inbox: Ignoring error\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:34:51 WARN Executor: Issue communicating with driver in heartbeater\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n","\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n","\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n","\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n","\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n","\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n","\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n","\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n","\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n","\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n","\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n","\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n","\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n","\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n","\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n","\t... 3 more\n","Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.10:52734\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n","\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n","\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n","\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n","\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n","\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n","\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n","\tat scala.concurrent.Future.flatMap(Future.scala:306)\n","\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n","\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n","\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n","\t... 17 more\n","24/06/01 22:34:51 ERROR Executor: Exit as unable to send heartbeats to driver more than 60 times\n"]}],"source":["from pyspark.sql.functions import col\n","\n","# Apply a filter to select records where the employee's name contains the letter 'o'\n","filtered_df = employees_df.filter(col(\"Emp_Name\").like(\"%o%\"))\n","\n","# Display the filtered DataFrame\n","filtered_df.show()\n"]},{"cell_type":"markdown","id":"5be76f4d-e897-4824-b2e5-1b161f471571","metadata":{},"source":["# Congratulations! You have completed the project.\n","\n","Now you know how to create a DataFrame from a CSV data file and perform a variety of DataFrame transformations and actions using Spark SQL.\n"]},{"cell_type":"markdown","id":"e76d7c60-1a58-4221-acc1-c3f6a1898ce4","metadata":{},"source":["## Authors\n"]},{"cell_type":"markdown","id":"52cb142e-2920-434e-b55b-0c0d3259d24a","metadata":{},"source":["Raghul Ramesh\n"]},{"cell_type":"markdown","id":"ed9b5df3-e990-4439-9b9b-86726db76279","metadata":{},"source":["Lavanya T S\n"]},{"cell_type":"markdown","id":"f20d3c8d-7d65-46a4-82e1-2c0045bd7588","metadata":{},"source":["<!--## Change Log -->\n"]},{"cell_type":"markdown","id":"4718a6ef-c843-47ea-83f3-ea8140e03fda","metadata":{},"source":["<!--|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n","|-|-|-|-|\n","|2023-09-01|0.1|Lavanya T S|Initial version|\n","|2023-09-11|0.2|Pornima More|QA pass with edits|-->\n"]},{"cell_type":"markdown","id":"f07b336a-9ecf-4367-9cb9-6ab1986db80f","metadata":{},"source":["<h3 align=\"center\"> &#169; IBM Corporation. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"prev_pub_hash":"aff0a6b4ec3a9cf15ae5d70a5c7ecac07e8a7f43b412a755232c9c99d1062fc8"},"nbformat":4,"nbformat_minor":4}
