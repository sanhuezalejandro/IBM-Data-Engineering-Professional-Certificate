{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e444405",
   "metadata": {},
   "source": [
    "# Hands-on lab on Hadoop Cluster\n",
    "\n",
    "[https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/ungradedLti/RVAek/hands-on-lab-hadoop-cluster-optional](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/ungradedLti/RVAek/hands-on-lab-hadoop-cluster-optional)\n",
    "\n",
    "![https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/IDSN-logo.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/IDSN-logo.png)\n",
    "\n",
    "### **What is a Hadoop Cluster?**\n",
    "\n",
    "A Hadoop cluster is a collection of computers, known as nodes, that are networked together to perform parallel computations on big data sets. The Name node is the master node of the Hadoop Distributed File System (HDFS). It maintains the meta data of the files in the RAM for quick access. An actual Hadoop Cluster setup involves extensives resources which are not within the scope of this lab. In this lab, you will use dockerized hadoop to create a Hadoop Cluster which will have:\n",
    "\n",
    "1. Namenode\n",
    "2. Datanode\n",
    "3. Node Manager\n",
    "4. Resource manager\n",
    "5. Hadoop history server\n",
    "\n",
    "## **Objectives**\n",
    "\n",
    "- Run a dockerized Cluster Hadoop instance\n",
    "- Create a file in the HDFS and view it on the GUI\n",
    "\n",
    "### **Set up Cluster Nodes Dockerized Hadoop**\n",
    "\n",
    "1. Start a new terminal\n",
    "\n",
    "![https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/New_terminal.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/New_terminal.png)\n",
    "\n",
    "1. Clone the repository to your theia environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b0f420b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "git clone https://github.com/ibm-developer-skills-network/ooxwv-docker_hadoop.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af090f17",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Navigate to the docker-hadoop directory to build it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "44798af3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "cd ooxwv-docker_hadoop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c2f795",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Compose the docker application.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46bf861c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "docker-compose up -d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57b76da",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> Compose is a tool for defining and running multi-container Docker applications. It uses the YAML file to configure the serives and enables us to create and start all the services from just one configurtation file.\n",
    ">\n",
    "\n",
    "You will see that all the five containers are created and started.\n",
    "\n",
    "![https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/all_containers.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/all_containers.png)\n",
    "\n",
    "1. Run the namenode as a mounted drive on bash.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "684f1beb",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "docker exec -it namenode /bin/bash\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a2c0f0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. You will observe that the prompt changes as shown below.\n",
    "\n",
    "![https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/root_prompt.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/root_prompt.png)\n",
    "\n",
    "# **Explore the hadoop environment**\n",
    "\n",
    "As you have learnt in the videos and reading thus far in the course, a Hadoop environment is configured by editing a set of configuration files:\n",
    "\n",
    "- **hadoop-env.sh** Serves as a master file to configure YARN, HDFS, MapReduce, and Hadoop-related project settings.\n",
    "- **core-site.xml** Defines HDFS and Hadoop core properties\n",
    "- **hdfs-site.xml** Governs the location for storing node metadata, fsimage file and log file.\n",
    "- **mapred-site-xml** Lists the parameters for MapReduce configuration.\n",
    "- **yarn-site.xml** Defines settings relevant to YARN. It contains configurations for the Node Manager, Resource Manager, Containers, and Application Master.\n",
    "\n",
    "For the docker image, these xml files have been configured already. You can see these in the directory **/opt/hadoop-3.2.1/etc/hadoop/** by running\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71d1518f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "ls /opt/hado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e6149",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Create a file in the HDFS**\n",
    "\n",
    "1. In the HDFS, create a directory structure named `user/root/input`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6296958e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "hdfs dfs -mkdir -p /user/root/input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd67054",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Copy all the hadoop configuration xml files into the input directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b7e8a13a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "hdfs dfs -put $HADOOP_HOME/etc/hadoop/*.xml /user/root/input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b210b0d9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Create a `data.txt` file in the current directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "924153ca",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "curl https://raw.githubusercontent.com/ibm-developer-skills-network/ooxwv-docker_hadoop/master/SampleMapReduce.txt --output data.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bbc440",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Copy the `data.txt` file into `/user/root`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e194ec6",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "hdfs dfs -put data.txt /user/root/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fd4c5b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Check if the file has been copied into the HDFS by viewing its content.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9536dab2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "hdfs dfs -cat /u\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b866955b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **iew the HDFS**\n",
    "\n",
    "1. Click the button below or click on the Skills Network button on the left, it will open the “Skills Network Toolbox”. Then click the Other then Launch Application. From there you should be able to enter the port number as `9870` and launch.\n",
    "\n",
    "View HDFS\n",
    "\n",
    "![https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/Launch_Application--new_IDE.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/Launch_Application--new_IDE.png)\n",
    "\n",
    "1. This will open up the Graphical User Interface (GUI) of the Hadoop node. Click on `Utilities` **>** `Broswe the file system` to browse the files.\n",
    "\n",
    "![https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/browse_filesystem.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/browse_filesystem.png)\n",
    "\n",
    "1. View the files in the directories that you have just created by clicking on `user` then `root`.\n",
    "\n",
    "![https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/Click-User.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/Click-User.png)\n",
    "\n",
    "![https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/Click-root.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/Click-root.png)\n",
    "\n",
    "![https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/browse_directory.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/browse_directory.png)\n",
    "\n",
    "1. Notice that the block size is 128 MB though the file size is actually much smaller. This is because the default block size used by HDFS is 128 MB.\n",
    "2. You can click on the file to check the file into. It gives you information about the file in terms of number of bytes, block id etc.,\n",
    "\n",
    "![https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/file_info.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/file_info.png)\n",
    "\n",
    "### **Congratulations! You have:**\n",
    "\n",
    "- Deployed Hadoop using Docker\n",
    "- Created data in HDFS and viewed it on the GUI\n",
    "\n",
    "\n",
    "\n",
    "## **Author(s)**\n",
    "\n",
    "Lavanya T S\n",
    "\n",
    "### **© IBM Corporation. All rights reserved.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe74585",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
