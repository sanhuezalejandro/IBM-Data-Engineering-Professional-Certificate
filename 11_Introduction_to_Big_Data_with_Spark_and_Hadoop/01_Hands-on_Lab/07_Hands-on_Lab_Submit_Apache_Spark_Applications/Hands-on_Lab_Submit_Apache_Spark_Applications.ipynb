{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f017cbe",
   "metadata": {},
   "source": [
    "# Hands-on Lab: Submit Apache Spark Applications\n",
    "\n",
    "![https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/IDSN-logo.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/IDSN-logo.png)\n",
    "\n",
    "**Estimated time needed: 20 minutes**\n",
    "\n",
    "In this lab, you will learn how to submit Apache Spark applications from a python script. This exercise is straightforward thanks to Docker Compose.\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "In this lab, you will:\n",
    "\n",
    "- Install a Spark Master and Worker using Docker Compose\n",
    "- Create a python script containing a spark job\n",
    "- Submit the job to the cluster directly from python (Note: you’ll learn how to submit a job from the command line in the Kubernetes Lab)\n",
    "\n",
    "## **Prerequisites**\n",
    "\n",
    "Note: If you are running this lab within the Skillsnetwort Lab environment, all prerequisites are already installed for you\n",
    "\n",
    "The only pre-requisites to this lab are:\n",
    "\n",
    "- A working *docker* installation\n",
    "- Docker Compose\n",
    "- The *git* command line tool\n",
    "- A python development environment\n",
    "\n",
    "# **Install a Apache Spark cluster using Docker Compose**\n",
    "\n",
    "On the right hand side to this instructions you’ll see the Cloud IDE. Select the *Lab* tab. On the menu bar select *Terminal>New Terminal*.\n",
    "\n",
    "![https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/NewTerminal.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/NewTerminal.png)\n",
    "\n",
    "Get the latest code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6ee277",
   "metadata": {},
   "outputs": [],
   "source": [
    "git clone https://github.com/big-data-europe/docker-spark.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81141d20",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Change the directory to the downloaded code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b3979",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd docker-spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6113ec32",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Start the cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f99804",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker-compose up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26674707",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "After quite some time you should see the following message:\n",
    "\n",
    "*Successfully registered with master spark://<server address>:7077*\n",
    "\n",
    "![https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/reg_success.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/reg_success.png)\n",
    "\n",
    "> NOTE: Please keep this terminal running and do not close it, as it is essential for further steps in the lab to run correctly.\n",
    ">\n",
    "\n",
    "# **Create code**\n",
    "\n",
    "1. Click `Terminal` from the menu, and click `New Terminal` to open a new terminal window.\n",
    "\n",
    "![https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/NewTerminal.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/NewTerminal.png)\n",
    "\n",
    "1. Once the terminal opens up at the bottom of the window, please type in the following command in the terminal to create the Python script.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4061b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "touch submit.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77072c0d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "A new python file called `submit.py` is created.\n",
    "\n",
    "1. Open the file in the file editor.\n",
    "\n",
    "![https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/edit_submitpy.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/edit_submitpy.png)\n",
    "\n",
    "1. Paste the following code to the file and save.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb66661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType\n",
    "\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster('spark://localhost:7077'))\n",
    "sc.setLogLevel(\"INFO\")\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.createDataFrame(\n",
    "[\n",
    "(1, \"foo\"),\n",
    "(2, \"bar\"),\n",
    "],\n",
    "StructType(\n",
    "[\n",
    "StructField(\"id\", IntegerType(), False),\n",
    "StructField(\"txt\", StringType(), False),\n",
    "]\n",
    "),\n",
    ")\n",
    "print(df.dtypes)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3699bd74",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# **Execute code / submit Spark job**\n",
    "\n",
    "Now we execute the python file we saved earlier.\n",
    "\n",
    "1. In the terminal, run the following commands to upgrade the pip installer to ensure you have the latest version by running the following commands.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b6b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -r ~/.cache/pip/selfcheck/\n",
    "pip3 install --upgrade pip\n",
    "pip install --upgrade distro-info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbe17cb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> rm -r ~/.cache/pip/selfcheck/ removes any previously cached version of pip and allows to install the latest one.\n",
    ">\n",
    "1. Please enter the following commands in the terminal to download the spark environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aec863",
   "metadata": {},
   "outputs": [],
   "source": [
    "wget https://archive.apache.org/dist/spark/spark-3.3.3/spark-3.3.3-bin-hadoop3.tgz && tar xf spark-3.3.3-bin-hadoop3.tgz && rm -rf spark-3.3.3-bin-hadoop3.tgz\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc24b56",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> This takes a while. This downloads the spark as a zipped archive and unzips it in the current directory.\n",
    ">\n",
    "1. Run the following commands to set up the `JAVA_HOME` which is preinstalled in the environment and `SPARK_HOME` which you just downloaded.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1b1ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "export JAVA_HOME=/usr/lib/jvm/java-1.11.0-openjdk-amd64\n",
    "export SPARK_HOME=/home/project/spark-3.3.3-bin-hadoop3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9917c9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Install the required packages to set up the spark environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30553173",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4af5876",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a9514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 -m pip install findspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b090f4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Type in the following command in the terminal to execute the Python script.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1fae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 submit.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90967395",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# **Experiment yourself**\n",
    "\n",
    "Please have a look at the UI of the Apache Spark master and worker.\n",
    "\n",
    "1. Click on the button below to launch the `Spark Master`. Alternatively, click on the Skills Network button on the left, it will open the “Skills Network Toolbox”. Then click the `Other`, then `Launch Application`. From there you should be able to enter the port number as `8080` and launch.\n",
    "\n",
    "Spark Master\n",
    "\n",
    "![https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/Launch_Application--new_IDE.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/Launch_Application--new_IDE.png)\n",
    "\n",
    "1. This will take you to the admin UI of the Spark master (if your popup blocker doesn’t prevent it).\n",
    "\n",
    "![https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/spark_submit_lab_master_ui.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/spark_submit_lab_master_ui.png)\n",
    "\n",
    "1. Please notice that you can see all registered workers (one in this case) and submitted jobs (also one in this case)\n",
    "\n",
    "> Note: The way how the lab environment works you can’t click on links in the UI - in a real installation, this of course is possible.\n",
    ">\n",
    "1. Click the button below to open the `Spark Worker` on 8081. Alternatively, click on the Skills Network button on the left, it will open the “Skills Network Toolbox”. Then click the `Other`, then `Launch Application`. From there, you should be able to enter the port number as `8081` and launch.\n",
    "\n",
    "Spark Worker\n",
    "\n",
    "You should find your currently running job here as well.\n",
    "\n",
    "# **Summary**\n",
    "\n",
    "In this lab you’ve learned how to setup an experimental Apache Spark cluster on top of Docker Compose. You are now able to submit a Spark job directly from python code. In the Kubernetes lab you’ll learn how to subit Spark jobs from command line as well.\n",
    "\n",
    "## **Author(s)**\n",
    "\n",
    "Romeo Kienzler\n",
    "\n",
    "[Lavanya T S](https://www.linkedin.com/in/lavanya-sunderarajan-199a445/)\n",
    "\n",
    "### **© IBM Corporation. All rights reserved.**"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
