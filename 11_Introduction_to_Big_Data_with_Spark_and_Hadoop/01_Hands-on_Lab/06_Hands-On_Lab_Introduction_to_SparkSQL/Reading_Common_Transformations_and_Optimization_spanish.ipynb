{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0925431",
   "metadata": {},
   "source": [
    "# Lectura: Transformaciones Comunes y Técnicas de Optimización en Spark\n",
    "\n",
    "Cuando trabajas con DataFrames de PySpark para el procesamiento de datos, es importante conocer los dos tipos de transformaciones: estrechas y amplias. Las transformaciones estrechas en Spark funcionan dentro de las particiones sin reorganizar los datos entre ellas. Se aplican localmente a cada partición, evitando el intercambio de datos. Por otro lado, las transformaciones amplias en Spark implican redistribuir y reorganizar los datos entre las particiones, lo que a menudo conduce a operaciones más complejas y que consumen más recursos.\n",
    "\n",
    "Para entender mejor este concepto, echemos un vistazo a la siguiente ilustración.\n",
    "\n",
    "![https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/Picture1.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/Picture1.png)\n",
    "\n",
    "Dentro de las transformaciones estrechas, los datos se transfieren sin ejecutar operaciones de reorganización de datos.\n",
    "\n",
    "![https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/Picture2.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/Picture2.png)\n",
    "\n",
    "Las transformaciones amplias implican la reorganización de datos entre particiones\n",
    "\n",
    "**Ejemplos de transformaciones estrechas**\n",
    "\n",
    "Las transformaciones estrechas se pueden comparar con realizar operaciones simples en conjuntos de datos distintos. Considera tener varios tipos de datos en contenedores separados. Puedes realizar acciones en cada contenedor de datos o mover datos entre contenedores de forma independiente sin requerir interacción o transferencia. Ejemplos de transformaciones estrechas incluyen modificar piezas individuales de datos, seleccionar elementos específicos o combinar dos contenedores de datos.\n",
    "\n",
    "1. **Map:** Aplicar una función a cada elemento del conjunto de datos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0956f254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"MapExample\")\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "mapped_rdd = rdd.map(lambda x: x * 2)\n",
    "mapped_rdd.collect() # Output: [2, 4, 6, 8, 10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b061d48d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **Filter:** Seleccionar elementos basados en una condición específica.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fc8cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"FilterExample\")\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "filtered_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
    "filtered_rdd.collect() # Output: [2, 4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958d696f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**3. Union:** Combinar dos conjuntos de datos con el mismo esquema.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23d3c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"UnionExample\")\n",
    "rdd1 = sc.parallelize([1, 2, 3])\n",
    "rdd2 = sc.parallelize([4, 5, 6])\n",
    "union_rdd = rdd1.union(rdd2)\n",
    "union_rdd.collect() # Output: [1, 2, 3, 4, 5, 6]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395b5a95",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Ejemplos de transformaciones amplias**\n",
    "\n",
    "Las transformaciones amplias se pueden comparar con tareas realizadas en equipo donde se necesita información de diferentes grupos para concluir. Imagina que tienes un grupo de amigos, cada uno con una pieza de un rompecabezas. Para armar el rompecabezas, podrías necesitar intercambiar piezas entre tus amigos para que todo encaje. Estas tareas son un buen ejemplo de transformación amplia. Tales tareas pueden ser un poco más complicadas porque todos necesitan colaborar y mover piezas.\n",
    "\n",
    "1. **GroupBy:** Agregar datos basados en una clave específica.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e36bfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"GroupByExample\")\n",
    "data = [(\"apple\", 2), (\"banana\", 3), (\"apple\", 5), (\"banana\", 1)]\n",
    "rdd = sc.parallelize(data)\n",
    "grouped_rdd = rdd.groupBy(lambda x: x[0])\n",
    "sum_rdd = grouped_rdd.mapValues(lambda values: sum([v[1] for v in values]))\n",
    "sum_rdd.collect() # Output: [('apple', 7), ('banana', 4)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec98b45",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **Join:** Combinar dos conjuntos de datos basados en una clave común.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83977912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"JoinExample\")\n",
    "rdd1 = sc.parallelize([(\"apple\", 2), (\"banana\", 3)])\n",
    "rdd2 = sc.parallelize([(\"apple\", 5), (\"banana\", 1)])\n",
    "joined_rdd = rdd1.join(rdd2)\n",
    "joined_rdd.collect() # Output: [('apple', (2, 5)), ('banana', (3, 1))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31188a0e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**3. Sort:** Reorganizar los datos basados en un criterio específico.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a5fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"SortExample\")\n",
    "data = [4, 2, 1, 3, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "sorted_rdd = rdd.sortBy(lambda x: x, ascending=True)\n",
    "sorted_rdd.collect() # Output: [1, 2, 3, 4, 5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aa7d7a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Las transformaciones amplias son similares a reorganizar y redistribuir datos entre diferentes grupos. Imagina tener conjuntos de datos que quieres combinar u organizar de una nueva manera. Sin embargo, esta tarea no es tan sencilla como trabajar con un solo conjunto de datos. Necesitas coordinar y mover datos entre estos conjuntos, lo que implica más complejidad. Por ejemplo, fusionar dos conjuntos de datos basados en un atributo común requiere reorganizar los datos entre ellos, lo que la convierte en una transformación amplia en ingeniería de datos.\n",
    "\n",
    "**PySpark DataFrame: Transformaciones comunes basadas en reglas**\n",
    "\n",
    "La API de DataFrame en PySpark ofrece varias transformaciones basadas en reglas predefinidas. Estas transformaciones están diseñadas para mejorar cómo se ejecutan las consultas y aumentar el rendimiento general. Veamos algunas transformaciones comunes basadas en reglas.\n",
    "\n",
    "1. **Predicate pushdown:** Empujar las condiciones de filtrado más cerca de la fuente de datos antes de procesar para minimizar el movimiento de datos.\n",
    "2. **Constant folding:** Evaluar expresiones constantes durante la compilación de la consulta para reducir la computación durante el tiempo de ejecución.\n",
    "3. **Column pruning:** Eliminar columnas innecesarias del plan de consulta para mejorar la eficiencia del procesamiento.\n",
    "4. **Join reordering:** Reorganizar las operaciones de unión para minimizar el tamaño de los datos intermedios y mejorar el rendimiento de la unión.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22459bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrame:\n",
      "+-----+---+------+\n",
      "| name|age|gender|\n",
      "+-----+---+------+\n",
      "|  Bob| 30|     M|\n",
      "|Diana| 28|     F|\n",
      "+-----+---+------+\n",
      "\n",
      "Folded DataFrame:\n",
      "+-----+---------+\n",
      "| name|(age + 2)|\n",
      "+-----+---------+\n",
      "|  Bob|       32|\n",
      "|Diana|       30|\n",
      "+-----+---------+\n",
      "\n",
      "Pruned DataFrame:\n",
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "|  Bob|\n",
      "|Diana|\n",
      "+-----+\n",
      "\n",
      "Reordered Join DataFrame:\n",
      "+-------+---+------+-------------+\n",
      "|   name|age|gender|         city|\n",
      "+-------+---+------+-------------+\n",
      "|  Alice| 25|     F|     New York|\n",
      "|    Bob| 30|     M|San Francisco|\n",
      "|Charlie| 22|     M|  Los Angeles|\n",
      "+-------+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"RuleBasedTransformations\").getOrCreate()\n",
    "# Sample input data for DataFrame 1\n",
    "data1 = [\n",
    "(\"Alice\", 25, \"F\"),\n",
    "(\"Bob\", 30, \"M\"),\n",
    "(\"Charlie\", 22, \"M\"),\n",
    "(\"Diana\", 28, \"F\")\n",
    "]\n",
    "# Sample input data for DataFrame 2\n",
    "data2 = [\n",
    "(\"Alice\", \"New York\"),\n",
    "(\"Bob\", \"San Francisco\"),\n",
    "(\"Charlie\", \"Los Angeles\"),\n",
    "(\"Eve\", \"Chicago\")\n",
    "]\n",
    "# Create DataFrames\n",
    "columns1 = [\"name\", \"age\", \"gender\"]\n",
    "df1 = spark.createDataFrame(data1, columns1)\n",
    "columns2 = [\"name\", \"city\"]\n",
    "df2 = spark.createDataFrame(data2, columns2)\n",
    "# Applying Predicate Pushdown (Filtering)\n",
    "filtered_df = df1.filter(col(\"age\") > 25)\n",
    "# Applying Constant Folding\n",
    "folded_df = filtered_df.select(col(\"name\"), col(\"age\") + 2)\n",
    "# Applying Column Pruning\n",
    "pruned_df = folded_df.select(col(\"name\"))\n",
    "# Join Reordering\n",
    "reordered_join = df1.join(df2, on=\"name\")\n",
    "# Show the final results\n",
    "print(\"Filtered DataFrame:\")\n",
    "filtered_df.show()\n",
    "print(\"Folded DataFrame:\")\n",
    "folded_df.show()\n",
    "print(\"Pruned DataFrame:\")\n",
    "pruned_df.show()\n",
    "print(\"Reordered Join DataFrame:\")\n",
    "reordered_join.show()\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6059c118",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Técnicas de optimización utilizadas en Spark SQL**\n",
    "\n",
    "1. **Predicate pushdown:** Aplica un filtro a DataFrame \"df1\" para seleccionar solo las filas donde la columna \"edad\" sea mayor que 25.\n",
    "2. **Constant folding:** Realiza una operación aritmética en la columna \"edad\" en el folded_df, añadiendo un valor constante de 2.\n",
    "3. **Column pruning:** Selecciona solo la columna \"nombre\" en el pruned_df, eliminando columnas innecesarias del plan de consulta.\n",
    "4. **Join reordering:** Realiza una unión entre df1 y df2 en la columna \"nombre\", permitiendo que Spark reordene la unión para mejorar el rendimiento.\n",
    "\n",
    "**Técnicas de optimización basadas en costos en Spark**\n",
    "\n",
    "Spark emplea técnicas de optimización basadas en costos para mejorar la eficiencia de la ejecución de consultas. Estos métodos implican estimar y analizar los costos asociados con las consultas, lo que lleva a decisiones más informadas que resultan en un rendimiento mejorado.\n",
    "\n",
    "1. **Adaptive query execution:** Ajusta dinámicamente el plan de consulta durante la ejecución basándose en estadísticas en tiempo de ejecución para optimizar el rendimiento.\n",
    "2. **Cost-based join reordering:** Optimiza el orden de unión basado en los costos estimados de diferentes rutas de unión.\n",
    "3. **Broadcast hash join:** Optimiza las uniones de tablas pequeñas transmitiendo una tabla a todos los nodos, lo que reduce la redistribución de datos.\n",
    "4. **Shuffle partitioning and memory management:** EGestiona eficientemente la redistribución de datos durante operaciones como groupBy y agregación y optimiza el uso de la memoria.\n",
    "\n",
    "Al utilizar estos métodos, Spark se esfuerza por ofrecer capacidades de procesamiento de datos eficientes y escalables. Es esencial comprender la aplicación efectiva de estas transformaciones y optimizaciones para lograr el mejor rendimiento posible de las consultas y la utilización óptima de los recursos del sistema.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18c60e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Join DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------------+\n",
      "|   name|age|         city|\n",
      "+-------+---+-------------+\n",
      "|  Alice| 25|     New York|\n",
      "|    Bob| 30|San Francisco|\n",
      "|Charlie| 22|  Los Angeles|\n",
      "+-------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"CostBasedOptimization\").getOrCreate()\n",
    "# Sample input data for DataFrame 1\n",
    "data1 = [\n",
    "(\"Alice\", 25),\n",
    "(\"Bob\", 30),\n",
    "(\"Charlie\", 22),\n",
    "(\"Diana\", 28)\n",
    "]\n",
    "# Sample input data for DataFrame 2\n",
    "data2 = [\n",
    "(\"Alice\", \"New York\"),\n",
    "(\"Bob\", \"San Francisco\"),\n",
    "(\"Charlie\", \"Los Angeles\"),\n",
    "(\"Eve\", \"Chicago\")\n",
    "]\n",
    "# Create DataFrames\n",
    "columns1 = [\"name\", \"age\"]\n",
    "df1 = spark.createDataFrame(data1, columns1)\n",
    "columns2 = [\"name\", \"city\"]\n",
    "df2 = spark.createDataFrame(data2, columns2)\n",
    "# Enable adaptive query execution\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "# Applying Adaptive Query Execution (Runtime adaptive optimization)\n",
    "optimized_join = df1.join(df2, on=\"name\")\n",
    "# Show the optimized join result\n",
    "print(\"Optimized Join DataFrame:\")\n",
    "optimized_join.show()\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2518d3f8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "En este ejemplo, creamos dos DataFrames (**df1** y **df2**) con datos de entrada de muestra. Luego, activamos la función de ejecución de consultas adaptativas configurando el parámetro de configuración **\"spark.sql.adaptive.enabled\"** en **\"true\"**.La Ejecución de Consultas Adaptativas permite que Spark ajuste el plan de consulta durante la ejecución basándose en estadísticas en tiempo de ejecución.\n",
    "\n",
    "El código realiza una unión entre **df1** y **df2** en la columna \"name\". La ejecución de consultas adaptativas de Spark ajusta dinámicamente el plan de consulta basándose en estadísticas en tiempo de ejecución, lo que puede resultar en un mejor rendimiento.\n",
    "\n",
    "## **Author(s)**\n",
    "\n",
    "Raghul Ramesh\n",
    "\n",
    "![https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/footer%20logo.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/footer%20logo.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
