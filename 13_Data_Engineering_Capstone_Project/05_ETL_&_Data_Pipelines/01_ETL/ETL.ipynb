{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "640bc1e6",
   "metadata": {},
   "source": [
    "# Hands-on Lab: ETL\n",
    "\n",
    "https://www.coursera.org/learn/data-enginering-capstone-project/ungradedLti/AKfNP/hands-on-lab-etl\n",
    "\n",
    "# **Hands on Lab - ETL**\n",
    "\n",
    "![https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/labs/module%201/images/IDSNlogo.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/labs/module%201/images/IDSNlogo.png)\n",
    "\n",
    "Estimated time needed: **30** minutes.\n",
    "\n",
    "# **About This SN Labs Cloud IDE**\n",
    "\n",
    "This Skills Network Labs Cloud IDE provides a hands-on environment for course and project related labs. It utilizes Theia, an open-source IDE (Integrated Development Environment) platform, that can be run on desktop or on the cloud. To complete this lab, we will be using the Cloud IDE based on Theia and MySQL database running in a Docker container. You will also need an instance of DB2 running in IBM Cloud or PostgreSQL database running in a Docker container.\n",
    "\n",
    "# **Important Notice about this lab environment**\n",
    "\n",
    "Please be aware that sessions for this lab environment are not persistent. A new environment is created for you every time you connect to this lab. Any data you may have saved in an earlier session will get lost. To avoid losing your data, please plan to complete these labs in a single session.\n",
    "\n",
    "# **Scenario**\n",
    "\n",
    "You are a data engineer at an e-commerce company. You need to keep data synchronized between different databases/data warehouses as a part of your daily routine. One task that is routinely performed is the sync up of staging data warehouse and production data warehouse. Automating this sync up will save you a lot of time and standardize your process. You will be given a set of python scripts to start with. You will use/modify them to perform the incremental data load from MySQL server which acts as a staging warehouse to the IBM DB2 or PostgreSQL which is a production data warehouse. This script will be scheduled by the data engineers to sync up the data between the staging and production data warehouse.\n",
    "\n",
    "# **Objectives**\n",
    "\n",
    "In this assignment you will write a python program that will:\n",
    "\n",
    "- Connect to IBM DB2 or PostgreSQL data warehouse and identify the last row on it.\n",
    "- Connect to MySQL staging data warehouse and find all rows later than the last row on the datawarehouse.\n",
    "- Insert the new data in the MySQL staging data warehouse into the IBM DB2 or PostgreSQL production data warehouse.\n",
    "\n",
    "# **Software Required**\n",
    "\n",
    "- MySQL Server\n",
    "- IBM DB2 or PostgreSQL\n",
    "\n",
    "# **Note - Screenshots**\n",
    "\n",
    "Throughout this lab you will be prompted to take screenshots and save them on your own device. You will need these screenshots to either answer graded quiz questions or to upload as your submission for peer review at the end of this course. You can use various free screengrabbing tools to do this or use your operating system's shortcut keys to do this (for example Alt+PrintScreen in Windows).\n",
    "\n",
    "# **Prepare the lab environment**\n",
    "\n",
    "Before you start the assignment:\n",
    "\n",
    "Step 1: Start MySQL server\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47c2a799",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "mysql -u root -p "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0abe482",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Step 2: Create a database named `sales`\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7563bc68",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "CREATE DATABASE sales;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444f3071",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Step 3: Download the file below\n",
    "\n",
    "[https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/ETL/sales.sql](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/ETL/sales.sql)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3066ecd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  341k  100  341k    0     0  11955      0  0:00:29  0:00:01  0:00:28 11947 0     0   181k      0  0:00:01  0:00:01 --:--:--  181k\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!curl -O https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/ETL/sales.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cc06344",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv sales.sql data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db92051",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Step 4: Import the data in the file `sales.sql` into the `sales` database.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50903eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e45a42",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Step 5: Verify that you can access your cloud instance of IBM DB2 server.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e1c25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287e2cb0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Step 6: Download the mysqlconnect.py python programs from link below.\n",
    "\n",
    "[https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/BFgMvlR8BKEjijGlWowK1Q/mysqlconnect.py](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/BFgMvlR8BKEjijGlWowK1Q/mysqlconnect.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fe16e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1132  100  1132    0     0    896      0  0:00:01  0:00:01 --:--:--   896\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/BFgMvlR8BKEjijGlWowK1Q/mysqlconnect.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb66fc0c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Step 7: mysqlconnect.py has the sample code to help you understand how to connect to MySQL using Python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e4dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b20da1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Step 8: Modify mysqlconnect.py suitably and make sure you are able to connect to the MySQL server instance on the Theia environment.\n",
    "\n",
    "> Note: Before executing mysqlconnect.py note that you install the connector using the command python3.11 -m pip install mysql-connector-python;\n",
    ">\n",
    "\n",
    "In order to complete the tasks below, you have the option to complete them on either a DB2 database (Option A) or on PostgreSQL (Option B).\n",
    "\n",
    "If you choose **Option A**, please follow tasks 9-12. If you choose **Option B**, please follow tasks 13-16.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63fe30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76282d1b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Option A: If you choose DB2 as the data warehouse:**\n",
    "\n",
    "Step 9: Download the db2connect.py python program from the link below.\n",
    "\n",
    "[https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/ETL/db2connect.py](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/ETL/db2connect.py)\n",
    "\n",
    "db2connect.py has the sample code to help you understand how to connect to the cloud instance of IBM DB2 using Python.\n",
    "\n",
    "> Note: Before executing db2connect.py note that you install the connector using the command pip install --force-reinstall ibm_db==3.1.0 ibm_db_sa==0.3.3\n",
    ">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "084bbadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2020  100  2020    0     0   2118      0 --:--:-- --:--:-- --:--:--  2117\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/ETL/db2connect.py && mv db2connect.py python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66490e86",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Step 10: Modify db2connect.py suitably and make sure you are able to connect to your cloud instance of IBM DB2 from the Theia environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64567065",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed19170",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Step 11: Download the file below\n",
    "\n",
    "[sales.csv](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/sales-csv3mo8i5SHvta76u7DzUfhiw.csv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce07aba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  533k  100  533k    0     0   279k      0  0:00:03  0:00:01  0:00:02  143k1  0:00:01 --:--:--  279k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/sales-csv3mo8i5SHvta76u7DzUfhiw.csv && mv sales-csv3mo8i5SHvta76u7DzUfhiw.csv data/sales.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615ba5c9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Step 12: Load sales.csv into a table named `sales_data` on your cloud instance of IBM DB2 database.\n",
    "\n",
    "> Note: By default, the sales.csv file contains price and timestamp columns, which are not present in sales.sql. Therefore, after loading the CSV into the sales_data table, you should run the script below. This script ensures that for any new values, the price column will be updated to 0 by default, and the timestamp will be updated with the current timestamp.\n",
    ">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbd62d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql\n",
    "ALTER TABLE sales_data ALTER COLUMN timestamp SET data type timestamp;\n",
    "ALTER TABLE sales_data ALTER COLUMN timestamp SET NOT NULL;\n",
    "ALTER TABLE sales_data ALTER COLUMN timestamp SET DEFAULT CURRENT_TIMESTAMP;\n",
    "\n",
    "ALTER TABLE sales_data ALTER COLUMN price SET data type decimal;\n",
    "ALTER TABLE sales_data ALTER COLUMN price SET DEFAULT 0;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec6da78",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "At any stage, if you encounter an error like - *SQL0668N Operation not allowed for reason code <reason-code> on table <table-name>*, you should execute the following line of code to initiate a reorganization process for the sales_data table.\n",
    "\n",
    "`call sysproc.admin_cmd('reorg table sales_data');`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cccfee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574ea17a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "OR\n",
    "\n",
    "### **Option B: If you choose PostgreSQL as the data warehouse:**\n",
    "\n",
    "Step 13: Download the postgresqlconnect.py python program from the link below.\n",
    "\n",
    "[https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/ETL/postgresqlconnect.py](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/ETL/postgresqlconnect.py)\n",
    "\n",
    "postgresqlconnect.py has the sample code to help you understand how to connect to the PostgreSql data warehouse using Python.\n",
    "\n",
    "> Note: Before executing postgresqlconnect.py note that you install the connector using the command python3 -m pip install psycopg2\n",
    ">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1ff0f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1615  100  1615    0     0   1741      0 --:--:-- --:--:-- --:--:--  1740\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/ETL/postgresqlconnect.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a3e51d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "  Downloading psycopg2-2.9.9.tar.gz (384 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.9/384.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: psycopg2\n",
      "  Building wheel for psycopg2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for psycopg2: filename=psycopg2-2.9.9-cp311-cp311-macosx_11_0_arm64.whl size=132019 sha256=a88e57d2e1b65da9ef73a954e819f6f2fa3de4f5f7b10bff3a4da02e437a8b79\n",
      "  Stored in directory: /Users/sanhuezalejandro/Library/Caches/pip/wheels/ab/34/b9/78ebef1b3220b4840ee482461e738566c3c9165d2b5c914f51\n",
      "Successfully built psycopg2\n",
      "Installing collected packages: psycopg2\n",
      "Successfully installed psycopg2-2.9.9\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install psycopg2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0b32b7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Step 14: Modify postgresqlconnect.py suitably and make sure you are able to connect to PostgreSql from the Theia environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e833d3f0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Step 15: Download the file below\n",
    "\n",
    "[sales.csv](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/sales-csv3mo8i5SHvta76u7DzUfhiw.csv)\n",
    "\n",
    "> Note: By default, the sales.csv file contains price and timestamp columns, which are not present in sales.sql. Therefore, you can use the below lines of code in your script to include price and timestamp columns when creating the table in Postgres.\n",
    ">\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb05d1ab",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "price decimal DEFAULT 0.0 NOT NULL,\n",
    "timeestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cc8fd1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Step 16: Create a table called `sales_data` using the columns `rowid`, `product_id`, `customer_id`, `price`, `quantity`\n",
    "\n",
    "`timeestamp`. Load sales.csv into the table `sales_data` on your PostgreSql database.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eb5731",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE sales_data (\n",
    "    rowid SERIAL PRIMARY KEY,\n",
    "    product_id INTEGER NOT NULL,\n",
    "    customer_id INTEGER NOT NULL,\n",
    "    price DECIMAL DEFAULT 0.0 NOT NULL,\n",
    "    quantity INTEGER NOT NULL,\n",
    "    timestamp TIMESTAMP WITHOUT TIME ZONE DEFAULT CURRENT_TIMESTAMP NOT NULL\n",
    ");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fadbe31",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Step 17: Download the `automation.py` from the following URL\n",
    "\n",
    "[https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/ETL/automation.py](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/ETL/automation.py)\n",
    "\n",
    "You will be using automation.py as a scafolding program to execute the tasks in this assignment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d3b2982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1445  100  1445    0     0   1449      0 --:--:-- --:--:-- --:--:--  1449\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/ETL/automation.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034ce4aa",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# **Exercise 1 - Automate loading of incremental data into the data warehouse**\n",
    "\n",
    "One of the routine tasks that is carried out around a data warehouse is the extraction of daily new data from the operational database and loading it into the data warehouse. In this exercise you will automate the extraction of incremental data, and loading it into the data warehouse.\n",
    "\n",
    "In order to complete Tasks 1 and 3 below, you have an option to complete the tasks on a DB2 database (Option A), or on PostgreSQL (Option B).\n",
    "\n",
    "### **Task 1 - Implement the function get_last_rowid()**\n",
    "\n",
    "In the program automation.py implement the function get_last_rowid()\n",
    "\n",
    "### **Option A: If you choose DB2 as the data warehouse:**\n",
    "\n",
    "This function must connect to the DB2 data warehouse and return the last rowid.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ea3df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e092d5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Option B: If you choose PostgreSQL as the data warehouse:**\n",
    "\n",
    "This function must connect to the PostgreSql as the data warehouse and return the last rowid.\n",
    "\n",
    "Take a screenshot of the python code clearly showing the implementation of the function get_last_rowid().\n",
    "\n",
    "Name the screenshot `get_last_rowid.jpg`. (Images can be saved with either the .jpg or .png extension.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "218e1feb",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def get_last_rowid():\n",
    "    query = \"SELECT MAX(rowid) FROM sales_data\"\n",
    "    postgres_cursor.execute(query)\n",
    "    result = postgres_cursor.fetchone()\n",
    "    return result[0] if result[0] is not None else 0\n",
    "\n",
    "\n",
    "last_row_id = get_last_rowid()\n",
    "print(\"Last row id on production datawarehouse = \", last_row_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c07e87",
   "metadata": {},
   "source": [
    "![](img/postgres/get_last_rowid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc5762a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Task 2 - Implement the function get_latest_records()**\n",
    "\n",
    "In the program automation.py implement the function get_latest_records()\n",
    "\n",
    "This function must connect to the MySQL database and return all records later than the given last_rowid.\n",
    "\n",
    "Take a screenshot of the python code clearly showing the implementation of the function get_latest_records().\n",
    "\n",
    "Name the screenshot `get_latest_records.jpg`. (Images can be saved with either the .jpg or .png extension.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cedb350e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "def get_latest_records(rowid):\n",
    "    query = \"SELECT * FROM sales_data WHERE rowid > %s\"\n",
    "    mysql_cursor.execute(query, (rowid,))\n",
    "    return mysql_cursor.fetchall()\n",
    "\n",
    "\n",
    "new_records = get_latest_records(last_row_id)\n",
    "print(\"New rows on staging datawarehouse = \", len(new_records))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3230c47f",
   "metadata": {},
   "source": [
    "![](img/postgres/get_latest_records.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c695e0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Task 3 - Implement the function insert_records()**\n",
    "\n",
    "In the program automation.py implement the function insert_records()\n",
    "\n",
    "### **Option A: If you choose DB2 as the data warehouse:**\n",
    "\n",
    "This function must connect to the DB2 data warehouse and insert all the given records.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7571612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca53319c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Option B: If you choose PostgreSQL as the data warehouse:**\n",
    "\n",
    "This function must connect to the PostgreSQL data warehouse and insert all the given records.\n",
    "\n",
    "Take a screenshot of the python code clearly showing the implementation of the function insert_records().\n",
    "\n",
    "Name the screenshot `insert_records.jpg`. (Images can be saved with either the .jpg or .png extension.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd191479",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def insert_records(records):\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO sales_data (rowid, product_id, customer_id, quantity, timestamp)\n",
    "    VALUES (%s, %s, %s, %s, CURRENT_TIMESTAMP)\n",
    "    \"\"\"\n",
    "    for record in records:\n",
    "        postgres_cursor.execute(insert_query, record)\n",
    "    postgres_conn.commit()\n",
    "\n",
    "\n",
    "insert_records(new_records)\n",
    "print(\"New rows inserted into production datawarehouse = \", len(new_records))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d96fe2",
   "metadata": {},
   "source": [
    "![](img/postgres/insert_records.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e4685",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Task 4 - Test the data synchronization**\n",
    "\n",
    "Run the program automation.py and test if the synchronization is happening as expected.\n",
    "\n",
    "Take a screenshot of the program output .\n",
    "\n",
    "Name the screenshot `synchronization.jpg`. (Images can be saved with either the .jpg or .png extension.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25efc201",
   "metadata": {},
   "source": [
    "![](img/postgres/synchronization.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda01277",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PosgreSQL Datawarehouse automation.py file\n",
    "\n",
    "# Import libraries required for connecting to mysql\n",
    "import mysql.connector\n",
    "\n",
    "# Import libraries required for connecting to DB2 or PostgreSql\n",
    "import psycopg2\n",
    "\n",
    "# Connect to MySQL\n",
    "mysql_conn = mysql.connector.connect(\n",
    "    user=\"root\", password=\"1234\", host=\"localhost\", database=\"sales\"\n",
    ")\n",
    "mysql_cursor = mysql_conn.cursor()\n",
    "\n",
    "# Connect to DB2 or PostgreSql\n",
    "postgres_conn = psycopg2.connect(\n",
    "    database=\"postgres\", user=\"postgres\", password=\"1234\", host=\"127.0.0.1\", port=\"5432\"\n",
    ")\n",
    "postgres_cursor = postgres_conn.cursor()\n",
    "\n",
    "# Find out the last rowid from DB2 data warehouse or PostgreSql data warehouse\n",
    "# The function get_last_rowid must return the last rowid of the table sales_data on the IBM DB2 database or PostgreSql.\n",
    "\n",
    "\n",
    "def get_last_rowid():\n",
    "    query = \"SELECT MAX(rowid) FROM sales_data\"\n",
    "    postgres_cursor.execute(query)\n",
    "    result = postgres_cursor.fetchone()\n",
    "    return result[0] if result[0] is not None else 0\n",
    "\n",
    "\n",
    "last_row_id = get_last_rowid()\n",
    "print(\"Last row id on production datawarehouse = \", last_row_id)\n",
    "\n",
    "# List out all records in MySQL database with rowid greater than the one on the Data warehouse\n",
    "# The function get_latest_records must return a list of all records that have a rowid greater than the last_row_id in the sales_data table in the sales database on the MySQL staging data warehouse.\n",
    "\n",
    "\n",
    "def get_latest_records(rowid):\n",
    "    query = \"SELECT * FROM sales_data WHERE rowid > %s\"\n",
    "    mysql_cursor.execute(query, (rowid,))\n",
    "    return mysql_cursor.fetchall()\n",
    "\n",
    "\n",
    "new_records = get_latest_records(last_row_id)\n",
    "print(\"New rows on staging datawarehouse = \", len(new_records))\n",
    "\n",
    "# Insert the additional records from MySQL into DB2 or PostgreSql data warehouse.\n",
    "# The function insert_records must insert all the records passed to it into the sales_data table in IBM DB2 database or PostgreSql.\n",
    "\n",
    "\n",
    "def insert_records(records):\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO sales_data (rowid, product_id, customer_id, quantity, timestamp)\n",
    "    VALUES (%s, %s, %s, %s, CURRENT_TIMESTAMP)\n",
    "    \"\"\"\n",
    "    for record in records:\n",
    "        postgres_cursor.execute(insert_query, record)\n",
    "    postgres_conn.commit()\n",
    "\n",
    "\n",
    "insert_records(new_records)\n",
    "print(\"New rows inserted into production datawarehouse = \", len(new_records))\n",
    "\n",
    "# disconnect from mysql warehouse\n",
    "mysql_cursor.close()\n",
    "mysql_conn.close()\n",
    "# disconnect from DB2 or PostgreSql data warehouse\n",
    "postgres_cursor.close()\n",
    "postgres_conn.close()\n",
    "# End of program\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a30415",
   "metadata": {},
   "outputs": [],
   "source": [
    "#db2 Datawarehouse automation.py file\n",
    "\n",
    "# Import libraries required for connecting to mysql\n",
    "import mysql.connector\n",
    "\n",
    "# Import libraries required for connecting to DB2\n",
    "import ibm_db\n",
    "\n",
    "# Connect to MySQL\n",
    "connection = mysql.connector.connect(user='root', password='OTAzMy1sb2NuZHNl',host='127.0.0.1',database='sales')\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Connect to DB2\n",
    "dsn_hostname = \"2f3279a5-73d1-4859-88f0-a6c3e6b4b907.c3n41cmd0nqnrk39u98g.databases.appdomain.cloud\" # e.g.: \"dashdb-txn-sbox-yp-dal09-04.services.dal.bluemix.net\"\n",
    "dsn_uid = \"prg79418\"                # e.g. \"abc12345\"\n",
    "dsn_pwd = \"QOKWifsXGuPcg5Li\"      # e.g. \"7dBZ3wWt9XN6$o0J\"\n",
    "dsn_port = \"30756\"                # e.g. \"50000\" \n",
    "dsn_database = \"bludb\"            # i.e. \"BLUDB\"\n",
    "dsn_driver = \"{IBM DB2 ODBC DRIVER}\" # i.e. \"{IBM DB2 ODBC DRIVER}\"           \n",
    "dsn_protocol = \"TCPIP\"            # i.e. \"TCPIP\"\n",
    "dsn_security = \"SSL\"              # i.e. \"SSL\"\n",
    "\n",
    "dsn = (\n",
    "    \"DRIVER={0};\"\n",
    "    \"DATABASE={1};\"\n",
    "    \"HOSTNAME={2};\"\n",
    "    \"PORT={3};\"\n",
    "    \"PROTOCOL={4};\"\n",
    "    \"UID={5};\"\n",
    "    \"PWD={6};\"\n",
    "    \"SECURITY={7};\").format(dsn_driver, dsn_database, dsn_hostname, dsn_port, dsn_protocol, dsn_uid, dsn_pwd, dsn_security)\n",
    "conn = ibm_db.connect(dsn, \"\", \"\")\n",
    "print (\"Connected to database: \", dsn_database, \"as user: \", dsn_uid, \"on host: \", dsn_hostname)\n",
    "\n",
    "# Find out the last rowid from DB2 data warehouse\n",
    "# The function get_last_rowid must return the \n",
    "# last rowid of the table sales_data on the IBM DB2 database.\n",
    "\n",
    "def get_last_rowid():\n",
    "    SQL = \"SELECT MAX(ROWID) FROM sales_data\"\n",
    "    stmt = ibm_db.exec_immediate(conn, SQL)\n",
    "    res = ibm_db.fetch_both(stmt)\n",
    "    print(res)\n",
    "    return int(res[0])\n",
    "\n",
    "last_row_id = get_last_rowid()\n",
    "print(\"Last row id on production datawarehouse = \", last_row_id)\n",
    "\n",
    "# List out all records in MySQL database with \n",
    "# rowid greater than the one on the Data warehouse\n",
    "# The function get_latest_records must return a \n",
    "# list of all records that have a rowid greater than \n",
    "# the last_row_id in the sales_data table in the \n",
    "# sales database on the MySQL staging data warehouse.\n",
    "\n",
    "def get_latest_records(rowid):\n",
    "    SQL = \"SELECT * FROM sales_data WHERE rowid > %s\"\n",
    "    cursor.execute(SQL, [rowid])\n",
    "    new_recs = cursor.fetchall()\n",
    "    # for row in new_recs: print(row)\n",
    "    return new_recs\n",
    "\n",
    "new_records = get_latest_records(last_row_id)\n",
    "\n",
    "print(\"New rows on staging datawarehouse = \", len(new_records))\n",
    "\n",
    "# Insert the additional records from MySQL into DB2 data warehouse.\n",
    "# The function insert_records must insert all the records passed\n",
    "# to it into the sales_data table in IBM DB2 database.\n",
    "\n",
    "def insert_records(records):\n",
    "    SQL = \"INSERT INTO sales_data(rowid,product_id,customer_id,quantity) VALUES(?,?,?,?);\"\n",
    "    stmt = ibm_db.prepare(conn, SQL)\n",
    "\n",
    "    for record in records:\n",
    "        ibm_db.execute(stmt, record)\n",
    "\n",
    "insert_records(new_records)\n",
    "print(\"New rows inserted into production datawarehouse = \", len(new_records))\n",
    "\n",
    "# disconnect from mysql warehouse\n",
    "connection.close()\n",
    "\n",
    "# disconnect from DB2 data warehouse\n",
    "ibm_db.close(conn)\n",
    "\n",
    "# End of program\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687f124d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# **Authors**\n",
    "\n",
    "Ramesh Sannareddy\n",
    "\n",
    "### **Other Contributors**\n",
    "\n",
    "Rav Ahuja\n",
    "\n",
    "## **Change Log**\n",
    "\n",
    "| Date (YYYY-MM-DD) | Version | Changed By | Change Description |\n",
    "| --- | --- | --- | --- |\n",
    "| 2023-06-29 | 0.7 | Lakshmi Holla | Updated PostgreSql |\n",
    "| 2023-05-11 | 0.6 | Eric Hao & Vladislav Boyko | Updated Page Frames |\n",
    "| 2023-05-10 | 0.5 | Eric Hao & Vladislav Boyko | Updated Page Frames |\n",
    "| 2023-05-10 | 0.4 | Eric Hao & Vladislav Boyko | Updated Page Frames |\n",
    "| 2021-13-12 | 0.1 | Ramesh Sannareddy | Created initial version |\n",
    "| 2022-09-29 | 0.2 | Appalabhaktula Hema | Updated code and instructions |\n",
    "| 2023-05-04 | 0.3 | Benny Li | Republished |\n",
    "\n",
    "### **© IBM Corporation 2023. All rights reserved.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
